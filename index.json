[{"categories":null,"content":"uniapp相关 官档地址 ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:0:0","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":null,"content":"1.基于vue3组合式布局 \u003ctemplate\u003e \u003c/template\u003e \u003cscript setup\u003e \u003c/script\u003e \u003cstyle lang=\"scss\"\u003e \u003c/style\u003e ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:1:0","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":null,"content":"2.新建页面 操作类似于微信小程序 ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:2:0","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":null,"content":"3.JS和vue等在uniapp中的语法 官档地址 ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:3:0","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":null,"content":"4.组件 官网相关教程 ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:4:0","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":null,"content":"基本使用和参数传递 根据官档所言，组件需要在根目录下创建一个名为camponents的文件夹，然后引用组件直接按照组件名的标签即可 下面是关于组件中的数据传输 比方说我用的组件是一个数据展示表格，那么就必须要当前页面把需要的数据从组件中获取或者传递给组件。 子组件 \u003cscript setup\u003e const props = defineProps([\"username\",\"avatar\"])// 声明接收的两个参数 console.log(\"props:\",props) \u003c/script\u003e \u003ctemplate\u003e \u003cview class=\"userinfo\"\u003e \u003cimage :src=\"avatar\" mode=\"\" class=\"avatar\"\u003e\u003c/image\u003e \u003cview class=\"username\"\u003e {{username}} \u003c/view\u003e \u003c/view\u003e \u003c/template\u003e 父组件 \u003ctemplate\u003e \u003cview class=\"content\"\u003e \u003cytw-headers\u003e\u003c/ytw-headers\u003e \u003c/view\u003e \u003cview class=\"\"\u003e \u003cUserInfo username=“123” avatar=“456”\u003e\u003c/UserInfo\u003e \u003c/view\u003e \u003c/template\u003e prors 其实有很多种写法，包括可一个它设置一些传入参数的限制，以及默认值的设定等,点击查看教程 ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:4:1","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":null,"content":"插槽 基本使用 插槽提供更加定制化的组件定制 官网插槽教程 子组件预留插槽位置，内容可以从父组件引用出插入 \u003ctemplate\u003e \u003cview\u003e \u003cview class=\"hander1\"\u003e1\u003c/view\u003e \u003cview class=\"main\"\u003e a \u003cslot\u003e\u003c!--通过插槽在这个组件中预留一个区域--\u003e\u003c/slot\u003e b \u003c/view\u003e \u003cview class=\"footer\"\u003e\u003c/view\u003e \u003c/view\u003e \u003c/template\u003e \u003cscript setup\u003e \u003c/script\u003e 父组件直接在标签中写入内容 \u003ctemplate\u003e \u003cytw-layout\u003e插槽插入内容\u003c/ytw-layout\u003e \u003c/template\u003e \u003cscript setup\u003e \u003c/script\u003e \u003cstyle lang=\"scss\"\u003e \u003c/style\u003e 效果如下 聚名插槽 假如页面的某个子组件内部需要使用多个插槽，直接通过上述方法是不够的，理论上我们需要给每一个插槽去一个名来标记，否则，所有的插槽都会被插入。 子组件写法如下 \u003ctemplate\u003e \u003cview\u003e \u003cview class=\"hander\"\u003e \u003cslot name=\"header\"\u003e\u003c!--通过插槽在这个组件中预留一个区域--\u003e\u003c/slot\u003e \u003c/view\u003e \u003cview class=\"main\"\u003e \u003cslot name='main'\u003e\u003c!--通过插槽在这个组件中预留一个区域--\u003e\u003c/slot\u003e \u003c/view\u003e \u003cview class=\"footer\"\u003e \u003cslot name=\"footer\"\u003e \u003c/slot\u003e \u003c/view\u003e \u003c/view\u003e \u003c/template\u003e 父组件写法 \u003ctemplate\u003e \u003cytw-layout\u003e \u003c!-- 可以直接追加插槽名称 --\u003e \u003ctemplate v-slot:header\u003e 头部内容..... \u003c/template\u003e \u003c!-- 第二种写法 --\u003e \u003ctemplate #main\u003e 主体内容...... \u003c/template\u003e \u003c/ytw-layout\u003e \u003c/template\u003e 这样就能根据插槽的名称，指定插槽的形式在父组件中插入内容 ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:4:2","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":null,"content":"emit声明事件触发 通过emit在子组件声明一个事件，它可以在父组件中声明并触发，从而实现将数据从子组件传递到父组件的功能 子组件 \u003ctemplate\u003e \u003cview\u003e 子组件内容！！！！！！ \u003cbutton @click=\"$emit('add',123)\"\u003e按钮1\u003c/button\u003e \u003c!-- 第二种写法 --\u003e \u003cbutton @click=\"doClick\"\u003e按钮2\u003c/button\u003e \u003c/view\u003e \u003c/template\u003e \u003cscript setup\u003e // 第二种写法声明事件名称 const emit = defineEmits([\"del\"]); function doClick(){ emit(\"del\",\"删除\") } \u003c/script\u003e \u003cstyle scoped\u003e \u003c/style\u003e # 可以看到子组件的按钮点击时间通过emit声明了一个add事件并且传递了参数123 父组件 \u003ctemplate\u003e \u003cytw-child @add=\"onAdd\"\u003e\u003c/ytw-child\u003e \u003c/template\u003e \u003cscript setup\u003e const onAdd = function(e){ console.log(e) } \u003c/script\u003e \u003cstyle lang=\"scss\"\u003e \u003c/style\u003e # 可以看到父组件上讲add事件的回调声明为onAdd，这样，一旦子组件点击后执行顺序如下 子组件触发点击事件=\u003e声明add事件=\u003e父组件上ytw-child标签上的add事件触发=\u003e触发add时间的回调onAdd 执行了打印e的动作，看到了传递的的参数123 ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:4:3","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":null,"content":"5 组件生命周期 使用vue3的生命周期就可以 常见四个阶段： 创建，挂载，更新，销毁 主要是在某些饥饿段和处理某些逻辑的代码。 ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:5:0","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":null,"content":"6 使用defineExpose暴露子组件的属性及方法 上述的组件中虽然可以通过emit来向外声明函数，但是外部依旧无法直接访问子组件内部的数据，因此vue提供的defineExpose就可以在uniapp中很好的进行导入了。 子组件 \u003ctemplate\u003e \u003cview class=\"layout\"\u003e ytw-demo组件 {{\"子组件count值\"+count}} \u003c/view\u003e \u003c/template\u003e \u003cscript setup\u003e import { ref } from 'vue'; const count = ref(100) const name = ref(\"余天王\") const obj = ref({ age: 111, price: 9999 }) function countAdd() { count.value++ } defineExpose({ count, name, obj, countAdd }) // 向外导出,以便父组件进行导出 \u003c/script\u003e \u003cstyle lang=\"scss\" scoped\u003e .layout{ background-color: green; height: 200px; } \u003c/style\u003e 父组件 \u003ctemplate\u003e \u003cview class=\"layout\"\u003e \u003cytw-demo ref=\"child1\"\u003e\u003c/ytw-demo\u003e \u003c!-- 子组件通过defineExpose暴露的数据都会通过这个child1暴露出来，只需要在js中定义同名变量，在页面挂载后就会将其自动赋值 --\u003e \u003cbutton @click=\"child1.countAdd\"\u003e父组件按钮\u003c/button\u003e \u003c/view\u003e \u003c/template\u003e \u003cscript setup\u003e import { onMounted, ref } from 'vue'; const child1 = ref(null); console.log(\"child.value:\",child1.value) // 直接打印因为组件还么有挂载，因此只是null onMounted(()=\u003e{ // 页面渲染后就会将对应的child暴露给父组件 // console.log(\"onMounted打印结果:\",child1.value) console.log(\"onMounted打印结果:\",child1.value.count) console.log(\"onMounted打印结果:\",child1.value.name) console.log(\"onMounted打印结果:\",child1.value.obj) console.log(\"onMounted打印结果:\",child1.value.countAdd) }) \u003c/script\u003e \u003cstyle lang=\"scss\"\u003e \u003c/style\u003e ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:6:0","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":null,"content":"7 页面生命周期 页面生命周期 ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:7:0","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":null,"content":"8.pages.json 页面路由 官档地址 下面是一些常见的配置 { \"pages\": [ //pages数组中第一项表示应用启动页，参考：https://uniapp.dcloud.io/collocation/pages { \"path\": \"pages/index/index\", \"style\": { \"navigationBarTitleText\": \"首页\" // \"enablePullDownRefresh\": true } }, { \"path\": \"pages/home/home\", \"style\": { \"navigationBarTitleText\": \"我的\" } }, { \"path\": \"pages/category/category\", \"style\": { \"navigationBarTitleText\": \"分类\" } } ], \"globalStyle\": { // 全局样式 \"navigationBarBackgroundColor\": \"#21f845\", \"navigationBarTextStyle\": \"black\", \"navigationBarTitleText\": \"YTW\", // \"navigationStyle\": \"custom\", // 隐藏导航栏 \"enablePullDownRefresh\": true, \"backgroundColor\": \"#CAF0DF\", \"onReachBottomDistance\": 50 }, \"tabBar\": { \"color\": \"#999\", \"selectedColor\": \"#69bbe6\", // \"backgroundColor\": \"#ccc\", \"borderStyle\": \"white\", \"fontSize\": 15, \"list\": [{ \"pagePath\": \"pages/index/index\", \"text\": \"首页\", \"iconPath\": \"./static/tabbar/home.png\", \"selectedIconPath\": \"./static/tabbar/home-h.png\" }, { \"pagePath\": \"pages/category/category\", \"text\": \"分类\", \"iconPath\": \"./static/tabbar/category.png\", \"selectedIconPath\": \"/static/tabbar/category-h.png\" }, { \"pagePath\": \"pages/home/home\", \"text\": \"个人中心\", \"iconPath\": \"./static/tabbar/self.png\", \"selectedIconPath\": \"./static/tabbar/self-h.png\" }] }, \"uniIdRouter\": {} } ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:8:0","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":null,"content":"9 下载第三方组件 ","date":"2024-05-27","objectID":"http://localhost:1313/posts/uniapp/:9:0","tags":null,"title":"Uniapp基础","uri":"http://localhost:1313/posts/uniapp/"},{"categories":["deploy"],"content":"关于Docker的基本使用","date":"2024-05-20","objectID":"http://localhost:1313/posts/docker/","tags":null,"title":"Docker容器相关","uri":"http://localhost:1313/posts/docker/"},{"categories":["deploy"],"content":"1 docker作用和介绍 docker 的作用主要可以帮助我们在一台电脑上创建出多个隔离的环境，比传统的虚拟机极大的节省资源 。类似于虚拟环境概念。 2 docker相关链接 DockerHub，远程镜像仓库。https://hub.docker.com/ 仓库中有官方的一些镜像文件，也有开发者自定义的镜像文件。 Images，本地镜像仓库。 可以去远程仓库下载镜像到本地仓库，后续再根据镜像为“模版”去创建容器。本地的镜像也可以发布到远程镜像库。 3 基本使用 ","date":"2024-05-20","objectID":"http://localhost:1313/posts/docker/:0:0","tags":null,"title":"Docker容器相关","uri":"http://localhost:1313/posts/docker/"},{"categories":["deploy"],"content":"安装docker-ce社区版 配置repo源 curl -o /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum clean all \u0026\u0026 yum makecache 查看可下载版本 yum list docker-ce --showduplicates | sort -r 安装docker # 最新版 yum install -y docker-ce # 指定某版本 yum install -y docker-ce-23.0.6 ","date":"2024-05-20","objectID":"http://localhost:1313/posts/docker/:1:0","tags":null,"title":"Docker容器相关","uri":"http://localhost:1313/posts/docker/"},{"categories":["deploy"],"content":"启动docker 设置开机启动 systemctl enable docker 启动docker systemctl start docker # 重启 systemctl restart docker 停止docker systemctl stop docker 其它命令 ## 查看docker信息 docker version ## 查看docker信息 docker info ## docker-client which docker ## docker daemon ps -ef |grep docker ","date":"2024-05-20","objectID":"http://localhost:1313/posts/docker/:2:0","tags":null,"title":"Docker容器相关","uri":"http://localhost:1313/posts/docker/"},{"categories":["deploy"],"content":"宿主机网卡转发 cat \u003c\u003cEOF \u003e /etc/sysctl.d/docker.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.conf.default.rp_filter = 0 net.ipv4.conf.all.rp_filter = 0 net.ipv4.ip_forward=1 EOF sysctl -p /etc/sysctl.d/docker.conf ","date":"2024-05-20","objectID":"http://localhost:1313/posts/docker/:3:0","tags":null,"title":"Docker容器相关","uri":"http://localhost:1313/posts/docker/"},{"categories":["deploy"],"content":"部署一个简单的项目 通过docker部署项目，主要有三个步骤： 1，下载基础的镜像； 2，构建自己的镜像; 3. 创建容器 需求：假设现在我有一个Flask程序，希望通过docker将他部署在ubuntu的操作系统上运行。 from flask import Flask app = Flask(__name__) @app.route(\"/index\") def index(): return \"欢迎光临红浪漫\" if __name__ == \"__main__\": app.run(host=\"0.0.0.0\",port=8000) ","date":"2024-05-20","objectID":"http://localhost:1313/posts/docker/:4:0","tags":null,"title":"Docker容器相关","uri":"http://localhost:1313/posts/docker/"},{"categories":["deploy"],"content":"下载基础镜像 [root@192 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE [root@192 ~]# [root@192 ~]# [root@192 ~]# docker pull ubuntu:18.04 [root@192 ~]# [root@192 ~]# 18.04: Pulling from library/ubuntu 284055322776: Pull complete Digest: sha256:0fedbd5bd9fb72089c7bbca476949e10593cebed9b1fb9edf5b79dbbacddd7d6 Status: Downloaded newer image for ubuntu:18.04 docker.io/library/ubuntu:18.04 [root@192 ~]# [root@192 ~]# [root@192 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 18.04 5a214d77f5d7 20 months ago 63.1MB [root@192 ~]# [root@192 ~]# 这个只有基础镜像，j接下来可以基础镜像的基础上创造出一个新的镜像：基础镜像 + 必备环境 + 代码。 后续基础新的镜像来创建容器并运行。 ","date":"2024-05-20","objectID":"http://localhost:1313/posts/docker/:4:1","tags":null,"title":"Docker容器相关","uri":"http://localhost:1313/posts/docker/"},{"categories":["deploy"],"content":"构建镜像 Dockerfile 就是集成docker环境初始化状态的一些命令集合，通过Dockerfile的语法转换成对应的linux命令，在容器创建完初期，执行这些内容，比如下载git ，python 等 编写Dockerfile # Base images 基础镜像 FROM ubuntu:18.04 #MAINTAINER 维护者信息 LABEL maintainer ytw@jjjj.com #RUN 执行以下命令 RUN apt update RUN apt install python3 python3-pip -y RUN pip3 install flask RUN mkdir -p /data/www/ #拷贝文件至工作目录 COPY app.py /data/www/app.py #工作目录 WORKDIR /data/www/ #EXPOSE 映射端口 EXPOSE 80 #容器启动时执行命令 CMD [\"python3\",\"app.py\"] 根据Dockerfile构建镜像 [root@192 crm]# docker build -t v0:0.1 . -f Dockerfile [+] Building 0.0s (12/12) FINISHED =\u003e [internal] load build definition from Dockerfile =\u003e =\u003e naming to docker.io/library/v1:0.01 0.0s ... =\u003e =\u003e naming to docker.io/library/v1:0.01 [root@192 crm]# [root@192 crm]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE v1 0.01 0dd632180107 3 minutes ago 513MB ubuntu 18.04 5a214d77f5d7 20 months ago 63.1MB centos latest 5d0da3dc9764 20 months ago 231MB ","date":"2024-05-20","objectID":"http://localhost:1313/posts/docker/:4:2","tags":null,"title":"Docker容器相关","uri":"http://localhost:1313/posts/docker/"},{"categories":["deploy"],"content":"容器 基于构建的镜像，来创建容器并启动。 [root@192 crm]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE v1 0.01 cd885cb29f41 9 minutes ago 513MB ubuntu 18.04 5a214d77f5d7 20 months ago 63.1MB [root@192 crm]# [root@192 crm]# [root@192 crm]# docker run -d -p 80:8000 cd885cb29f41 93352e1c52ec397233b33ca88545c3dd8ab9f6f2c3c93ad292944d8f4d570bc8 [root@192 crm]# [root@192 crm]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS 93352e1c52ec cd885cb29f41 \"python3 app.py\" 42 seconds ago Up 42 seconds ... [root@192 crm]# [root@192 crm]# docker ps [root@192 crm]# docker ps -a [root@192 crm]# docker stop 933 [root@192 crm]# docker rm 933 4 相关命令 让我来稍微介绍下上面涉及到的一些docker相关的命令 ","date":"2024-05-20","objectID":"http://localhost:1313/posts/docker/:4:3","tags":null,"title":"Docker容器相关","uri":"http://localhost:1313/posts/docker/"},{"categories":["deploy"],"content":"shell端的docker命令 查看已安装镜像 docker images 查看容器 docker ps # 查看正在运行的容器 docker ps -a # 查看所有的容器 下载镜像 docker pull ubuntu:18.04 # 下载了乌邦图18.04 镜像 构建镜像 docker build -t 镜像名:0.1 . -f Dockerfile 启动容器 docker run -d -p 80:8000 cd885cb29f41 # -d 表示后台启动，不占用当前终端 -p 表示指定端口转发，以宿主机80 转发到容器的8000端口 cbxxxx是镜像的id 删除镜像 docker rmi f16 停止/删除容器 #停止容器 docker stop xxx #删除容器 docker rm xxxx # 批量停止容器 docker stop `docker ps -aq` # 批量删除容器 docker rm `docker ps -aq` ","date":"2024-05-20","objectID":"http://localhost:1313/posts/docker/:5:0","tags":null,"title":"Docker容器相关","uri":"http://localhost:1313/posts/docker/"},{"categories":["deploy"],"content":"Dockerfile中的命令 指定镜像 # Base images 基础镜像 FROM ytw/centos76py39:1.0 #MAINTAINER 维护者信息 MAINTAINER ytw@live.com # git RUN yum install git -y RUN git config --global user.name \"ytw\" RUN git config --global user.email \"ytw@gmail.com\" # git拉代码 /data/blog WORKDIR /data/ RUN git clone https://gitee.com/ytw/blog.git # 虚拟环境 WORKDIR /data/blog/ RUN pip3.9 install virtualenv RUN virtualenv /envs/blog --python=python3.9 RUN /envs/blog/bin/pip3.9 install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple # 收集静态资源 /data/allstatic/ WORKDIR /data/blog/ RUN /envs/blog/bin/python manage.py collectstatic # 安装uwsgi RUN /envs/blog/bin/pip3.9 install uwsgi # 安装nginx WORKDIR /data/blog/ RUN rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm RUN yum install nginx -y # 易错点 RUN cp /data/blog/blog.conf /etc/nginx/myblog.conf # 运行项目 WORKDIR /data/blog # CMD [\"/envs/blog/bin/uwsgi\",\"--ini\",\"uwsgi.ini\"] # CMD [\"nginx\",\"-c\",\"/etc/nginx/myblog.conf\",\"-g\",\"daemon off;\"] CMD [\"/bin/sh\",\"run.sh\"] ","date":"2024-05-20","objectID":"http://localhost:1313/posts/docker/:6:0","tags":null,"title":"Docker容器相关","uri":"http://localhost:1313/posts/docker/"},{"categories":["default"],"content":"分享一些自己玩儿的项目 websocket协议实现简单的聊天平台 本博客网站源码地址 wx小程序校园管理系统 前端微信小程序代码 后端api 后台管理 定制后台管理系统 壁纸app uniapp前端代码 myfamily App uniapp前端代码 api地址 前后端不分离企业官网 源码地址 图书管理系统 毕设菜鸟版 课设更菜版 链接：https://pan.baidu.com/s/1Y3rIV9n_UXKd4reE3OhuiA 提取码：1234 –来自百度网盘超级会员V5的分享 爬虫脚本 美人图网(少儿不宜) 笔趣阁小说 彼岸图网4k壁纸(scrapy实现) 码字中….. ","date":"2024-04-20","objectID":"http://localhost:1313/posts/my_project/:0:0","tags":null,"title":"项目地址","uri":"http://localhost:1313/posts/my_project/"},{"categories":["default"],"content":"DRF整理 ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:0:0","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"1.认证组件 基本使用 单视图全方法应用 ​ 这个非常简单，直接看就能看懂，认证组件必须重写父类的authenticate方法。同时认证组件可以正常接收到请求对象，也就是请求中的所有内容，包括请求头和请求体，以及其它野数据，因此，在认证组件中进行校验逻辑，比如token是否有效等，如果通过校验，直接返回空值或者两个参数，这两个参数将会依次封装到此次请求的request对象的user和auth中。 class MineAuthentication(BaseAuthentication): def authenticate(self, request): token = request.query_params.get(\"token\") if not token: raise AuthenticationFailed({\"status\": 0, \"msg\": \"认证失败,没有登录\"}) user_object = models.User.objects.filter(token=token).first() if not user_object: raise AuthenticationFailed({\"status\": 0, \"msg\": \"token无效\"}) return user_object, token # request.user request.auth class User(APIView): authentication_classes = [MineAuthentication] def get(self,request): return Response(...) def post(self,request): return ... .... 单视图多方法应用 ​ 上述代码，可以在视图中的所有方法中应用对应的组件，但是当我们需要只针对某几个请求限制权限，应该如何做呢？ 这里可以使用闭包来完成这个需求，闭包的作用就是可以动态的调用一个函数，让这个返回我们想要的内容 def nb(cls, user_method): def inner(): obj = cls(user_method) return obj return inner 让我来解释一下这个函数，此函数将会接收两个，参数，然后最终返回一个inner ，而这个inner函数是在nb函数中进行声明的。因此，我们传入的cls 和method 将直接决定了这个函数的内容。 很显然，传入的cls可能是一个对象，那么他将会被实例化，同时传给构造方法的是user_method , 然后这个实例对象将是nb最终返回值。如果cls是一个函数，那么这个函数会被执行，并且函数结果作为nb返回值 那么我们可以自己写一个认证组件(它是一个类),在他的构造方法中，将会拿到user_method,此时将它进行封装，就可以在认证的方法中针对这个参数做不同的判断逻辑了 class My3Authentication(BaseAuthentication): def __init__(self, user_methods): self.user_methods = user_methods # 封装user_methods def authenticate(self, request): if request.method not in self.user_methods: # 如果请求方法不在对应的user_methods列表，直接跳过认证。 return .... def nb(cls, user_method): def inner(): obj = cls(user_method) return obj return inner class MyView(APIView): authentication_classes = [nb(My3Authentication, [\"GET\"])] # def get(self,request): return Response(...) 全局 # settings.py REST_FRAMEWORK = { \"DEFAULT_AUTHENTICATION_CLASSES\": [\"xxxxx\", ], # 全局应用认证组件 } ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:1:0","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"2.权限组件 权限组件听起来和认证类似，但是他们的应用场景有一定的区分度 认证组件一般适用： 用户是否登录 权限组件一般就是更加细致的划分 管理员权限 必须是xxx地区用户 必须大于xxx年龄等 基本使用（和认证组件的使用方法类似），需要写一个认证组件类，且重写父类has_permission方法，具有权限则返回True，如果没有权限，则返回False class MinePermission(BasePermission): message = {\"status\": False, \"msg\": \"无权限访问\"} # 认证失败返回的内容 code = \"500\" def has_permission(self, request, view): if request.user.level == 1: # 管理员 return True return False # 返回权限访问字符串 class User(APIView): permission_classes = [xxxx,xxxx] 全局应用 # settings.py REST_FRAMEWORK = { \"DEFAULT_AUTHENTICATION_CLASSES\": [\"xxxxx\", ], # 全局应用认证组件 \"DEFAULT_PERMISSION_CLASSES\": [\"utils.permission.MinePermission\"], # 全局应用权限组件 } ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:2:0","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"3.限流组件 限流，限制用户访问频率，例如：用户1分钟最多访问100次 或者 短信验证码一天每天可以发送50次， 防止盗刷。 对于匿名用户，使用用户IP作为唯一标识。 对于登录用户，使用用户ID或名称作为唯一标识。 用户访问记录可以记录在缓存中，因此可以配置redis pip install django-redis # settings.py CACHES = { \"default\": { \"BACKEND\": \"django_redis.cache.RedisCache\", \"LOCATION\": \"redis://127.0.0.1:6379\", \"OPTIONS\": { \"CLIENT_CLASS\": \"django_redis.client.DefaultClient\", \"PASSWORD\": \"qwe123\", } } } # views.py from rest_framework.views import APIView from rest_framework.response import Response from rest_framework import exceptions from rest_framework import status from rest_framework.throttling import SimpleRateThrottle from django.core.cache import cache as default_cache class ThrottledException(exceptions.APIException): status_code = status.HTTP_429_TOO_MANY_REQUESTS default_code = 'throttled' class MyRateThrottle(SimpleRateThrottle): cache = default_cache # 访问记录存放在django的缓存中（需设置缓存） scope = \"user\" # 构造缓存中的key cache_format = 'throttle_%(scope)s_%(ident)s' # 设置访问频率，例如：1分钟允许访问10次 # 其他：'s', 'sec', 'm', 'min', 'h', 'hour', 'd', 'day' THROTTLE_RATES = {\"user\": \"10/m\"} def get_cache_key(self, request, view): if request.user: ident = request.user.pk # 用户ID else: ident = self.get_ident(request) # 获取请求用户IP（去request中找请求头） # throttle_u # throttle_user_11.11.11.11ser_2 return self.cache_format % {'scope': self.scope, 'ident': ident} def throttle_failure(self): wait = self.wait() detail = { \"code\": 1005, \"data\": \"访问频率限制\", 'detail': \"需等待{}s才能访问\".format(int(wait)) } raise ThrottledException(detail) class OrderView(APIView): throttle_classes = [MyRateThrottle, ] def get(self, request): return Response({\"code\": 0, \"data\": \"数据...\"}) 让我来解释一下限流组件是如何工作的，他会维护一个列表，用户每次访问，都会将当时访问的时间按照队列的形式存入列表，同时它每次查询是否限流，会根据这个队列取规定时间内最近的几个，如果个数满了，那么接下来就会进行限制访问。 多个限流类 本质，每个限流的类中都有一个 allow_request 方法，此方法内部可以有三种情况： 返回True，表示当前限流类允许访问，继续执行后续的限流类。 返回False，表示当前限流类不允许访问，继续执行后续的限流类。所有的限流类执行完毕后，读取所有不允许的限流，并计算还需等待的时间。 抛出异常，表示当前限流类不允许访问，后续限流类不再执行。 全局配置 REST_FRAMEWORK = { \"DEFAULT_THROTTLE_CLASSES\":[\"xxx.xxx.xx.限流类\", ], \"DEFAULT_THROTTLE_RATES\": { \"user\": \"10/m\", \"xx\":\"100/h\" } } 关于限流源码 ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:3:0","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"4.解析器 drf中可以通过request.data直接获取请求体数据，而这个数据是在drf中对其进行的封装 ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:4:0","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"3.7.1 JSONParser （*） ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:4:1","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"3.7.2 FormParser ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:4:2","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"3.7.3 MultiPartParser（*） \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eTitle\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cform action=\"http://127.0.0.1:8000/test/\" method=\"post\" enctype=\"multipart/form-data\"\u003e \u003cinput type=\"text\" name=\"user\" /\u003e \u003cinput type=\"file\" name=\"img\"\u003e \u003cinput type=\"submit\" value=\"提交\"\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:4:3","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"3.7.4 FileUploadParser（*） 解析器可以设置多个，默认解析器： from rest_framework.views import APIView from rest_framework.response import Response from rest_framework.parsers import MultiPartParser, JSONParser, FormParser class UserView(APIView): def post(self, request): print(request.content_type) print(request.data) return Response(\"...\") ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:4:4","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"5.序列化器 drf中提供序列化器，主要做两件事： 对请求数据进行校验类似于form表单校验 对数据库查询的对象进行序列化。 ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:5:0","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"5.1 定义序列化器： serializer： ​ 下面是常见的字段 class IndexSerializers(serializers.Serializer): # x1 = serializers.CharField(source=\"title\") # x2 = serializers.CharField(source=\"number\") # 平凡的字段 id = serializers.IntegerField() real_name = serializers.CharField() # 时间字段 c_time = serializers.DateTimeField(format=\"%Y-%m-%d\") # 选择字段 gender = serializers.IntegerField() gender_text = serializers.CharField(source=\"get_gender_display\") # 如果遇到choice类型字段 gender_dict = serializers.SerializerMethodField() # 序列化器遇到这个字段会自动去寻找 get_字段名() 的方法，并将其返回值作为字段值。 # 自定义字段 test_field = serializers.SerializerMethodField() # 外键字段 depart_str = serializers.CharField(source=\"depart\") depart_id = serializers.IntegerField(source=\"depart.id\") depart_text = serializers.CharField(source=\"depart.title\") depart_xxxxx = serializers.SerializerMethodField() depart = DepartSerializers() # 多对多 MtM hobbies1 = serializers.SerializerMethodField() hobbies2 = HobbySerializers(many=True, source=\"hobbys\") # 因为有多条数据，别忘记many属性的true hobbys = HobbySerializers(many=True) def get_gender_dict(self, obj): # print(\"obj:\", obj) return { \"id\": obj.gender, \"text\": obj.get_gender_display(), } def get_test_field(self, obj): return (\"余天王自定义字段\") def get_depart_xxxxx(self, obj): # print(obj) return { \"id\": obj.depart.id, \"title\": obj.depart.title, \"number\": obj.depart.number, } def get_hobbies1(self, obj): print(1111) hobby_objects = obj.hobbys.all() return [{\"id\": x.id, \"text\": x.text} for x in hobby_objects] modelserializer 基于serializer ，但是它可以省略手写字段，或者尽可能少些字段。而且支持了一键保存 class IndexModelSerializers(serializers.ModelSerializer): class Meta: model = models.UserInfo # 根据模型类的字段类型生成对应的序列化器字段、 fields = \"__all__\" # 指定全部字段 # fields = [\"id\",\"title\"] # 指定某几个字段 # exclude = [\"title\"] # 排除某几个字段 ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:5:1","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"5.2 钩子方法 class InfoSerializer(serializers.Serializer): title = serializers.CharField(required=True, max_length=20, min_length=6) order = serializers.IntegerField(required=False, max_value=100, min_value=10) level = serializers.ChoiceField(choices=[(1, \"高级\"), (2, \"中级\")]) # 正则 # email = serializers.EmailField() email = serializers.CharField(required=False, validators=[EmailValidator(message=\"邮箱格式错误！！！\")]) # EmailValidator 是django中提供的邮箱正则校验 more = serializers.CharField(required=False, validators=[RegexValidator(r\"\\d+\", message=\"格式错误\")]) code = serializers.CharField(required=False) # 钩子方法 def validate_code(self, value): print(\"code校验中...\") if len(value) \u003e 6: raise Exc.ValidationError(\"code字段长度不能大于6\") return value def validate(self, attrs): # raise Exc.ValidationError(\"整体校验失败\") return attrs 让我来解释一下，validate_xxxx 是针对某个字段进行校验，最后需要进行返回这个字段值，如果没有通过校验则返回校验异常，而被返回的值会被封装到self.validated_data 中。而validate是对于所有字段校验完毕后再来一次整体的校验。 ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:5:2","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"5.3 数据序列化以及校验 class InfoView(MineApiView): def post(self, request): ser = Info2Serializer(data=request.data) if ser.is_valid(): instance = ser.save() # ModelSerializer 自带的数保存功能,如果用户提交的数据比orm的少，可以手动加几个进去 ser.save(x1=xxx,x2=xxx) return Response(ser.data) return Response(ser.errors) 值得注意的是，前面我们在校验方法中抛出的异常并不会真正的抛出，而是被序列化器自己处理掉，如果希望抛出到视图函数层面的话仅需在is_valid()中加入参数即可，而加入参数后，异常会被视图函数层面捕获到，也就不会往下走了，所以这样修改甚至不需要if条件 class InfoView(MineApiView): def post(self, request): ser = Info2Serializer(data=request.data) ser.is_valid(): instance = ser.save(raise_exception=True) # 如果没有通过，此处就会报错，当然会被试图层面的dispath方法捕获到，打包成错误信息返回，效果和前面一致。 instance = ser.save() # ModelSerializer 自带的数保存功能,如果用户提交的数据比orm的少，可以手动加几个进去 ser.save(x1=xxx,x2=xxx) return Response(ser.data) ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:5:3","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"6.自定义异常返回 # settings.py REST_FRAMEWORK = { 'EXCEPTION_HANDLER': 'utils.exc_views.exception_handler', } from django.http import Http404 from rest_framework import exceptions from rest_framework.response import Response from rest_framework.exceptions import ValidationError from rest_framework.exceptions import Throttled from rest_framework.exceptions import PermissionDenied from rest_framework.exceptions import NotAuthenticated from rest_framework.exceptions import AuthenticationFailed from rest_framework.views import set_rollback from rest_framework.exceptions import APIException class ExtraException(APIException): pass def exception_handler(exc, context): if isinstance(exc, Http404): exc = exceptions.NotFound() exc.ret_code = 2001 elif isinstance(exc, PermissionDenied): exc = exceptions.PermissionDenied() exc.ret_code = 2002 elif isinstance(exc, (AuthenticationFailed, NotAuthenticated)): exc.ret_code = 2003 elif isinstance(exc, Throttled): exc.ret_code = 2004 elif isinstance(exc, ValidationError): exc.ret_code = 2005 # ... if isinstance(exc, exceptions.APIException): headers = {} if getattr(exc, 'auth_header', None): headers['WWW-Authenticate'] = exc.auth_header if getattr(exc, 'wait', None): headers['Retry-After'] = '%d' % exc.wait # if isinstance(exc.detail, (list, dict)): # data = exc.detail # else: # exc_code = getattr(exc, 'ret_code', None) or -1 # data = {'code': exc_code, 'detail': exc.detail} exc_code = getattr(exc, 'ret_code', None) or -1 data = {'code': exc_code, 'detail': exc.detail} set_rollback() return Response(data, status=exc.status_code, headers=headers) # return None data = {'code': -1, 'detail': str(exc)} return Response(data, status=500) ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:6:0","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"7 自定义返回值 from django.shortcuts import render from rest_framework.views import APIView from rest_framework.response import Response from django.shortcuts import HttpResponse from rest_framework.views import exception_handler from rest_framework.authentication import BaseAuthentication from rest_framework.exceptions import AuthenticationFailed from utils.exc_views import ExtraException class MineAPIView(APIView): def finalize_response(self, request, response, *args, **kwargs): response = super().finalize_response(request, response, *args, **kwargs) # 1.非正常返回 if response.exception: return response # 2.正常数据返回 response.data = {\"code\": 0, \"data\": response.data} return response 接下来让视图先继承我们自己写的类，它在寻找这个finalize_response方法的时候，就会被我们写的所覆盖掉。 ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:7:0","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"8 分页组件 drf提供的几个默认分页组件 BasePagination，分页基类 PageNumberPagination(BasePagination) 支持 /accounts/?page=4\u0026page_size=100 格式的分页 LimitOffsetPagination(BasePagination) 支持 ?offset=100\u0026limit=10 格式的分页 CursorPagination(BasePagination) 支持 上一下 \u0026 下一页 格式的分页（不常用） ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:8:0","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"PageNumberPagination ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:8:1","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"LimitOffsetPagination ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:8:2","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"9 条件搜索 ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:9:0","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"自定义Filter # urls.py from django.urls import path from app01 import views urlpatterns = [ path('api/users/', views.UserView.as_view( {\"get\": \"list\", \"post\": \"create\"} )), path('api/users/\u003cint:pk\u003e/', views.UserView.as_view( {\"get\": \"retrieve\", \"put\": \"update\", \"patch\": \"partial_update\", \"delete\": \"destroy\"} )), ] from rest_framework import serializers from rest_framework.viewsets import ModelViewSet from rest_framework.filters import BaseFilterBackend from app01 import models class UserModelSerializer(serializers.ModelSerializer): level_text = serializers.CharField( source=\"get_level_display\", read_only=True ) extra = serializers.SerializerMethodField(read_only=True) class Meta: model = models.UserInfo fields = [\"username\", \"age\", \"email\", \"level_text\", \"extra\"] def get_extra(self, obj): return 666 class Filter1(BaseFilterBackend): def filter_queryset(self, request, queryset, view): age = request.query_params.get('age') if not age: return queryset return queryset.filter(age=age) class Filter2(BaseFilterBackend): def filter_queryset(self, request, queryset, view): user_id = request.query_params.get('id') if not user_id: return queryset return queryset.filter(id__gt=user_id) class UserView(ModelViewSet): filter_backends = [Filter1, Filter2] queryset = models.UserInfo.objects.all() serializer_class = UserModelSerializer def perform_create(self, serializer): \"\"\" 序列化：对请求的数据校验成功后，执行保存。\"\"\" serializer.save(depart_id=1, password=\"123\") ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:9:1","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"10.视图 ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:10:0","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"APIView View，django APIView，drf，在请求到来时，新增了：免除csrf、请求封装、版本、认证、权限、限流的功能。 class GenericAPIView(APIView): pass # 10功能 class GenericViewSet(xxxx.View-2个功能, GenericAPIView): pass # 5功能能 class UserView(GenericViewSet): def get(self,request): pass APIView是drf中 “顶层” 的视图类，在他的内部主要实现drf基础的组件的使用，例如：版本、认证、权限、限流等。 from django.urls import path, re_path, include from app01 import views urlpatterns = [ path('api/users/', views.UserView.as_view()), path('api/users/\u003cint:pk\u003e/', views.UserDetailView.as_view()), ] from rest_framework.views import APIView from rest_framework.response import Response class UserView(APIView): # 认证、权限、限流等 def get(self, request): # 业务逻辑：查看列表 return Response({\"code\": 0, 'data': \"...\"}) def post(self, request): # 业务逻辑：新建 return Response({'code': 0, 'data': \"...\"}) class UserDetailView(APIView): # 认证、权限、限流等 def get(self, request,pk): # 业务逻辑：查看某个数据的详细 return Response({\"code\": 0, 'data': \"...\"}) def put(self, request,pk): # 业务逻辑：全部修改 return Response({'code': 0, 'data': \"...\"}) def patch(self, request,pk): # 业务逻辑：局部修改 return Response({'code': 0, 'data': \"...\"}) def delete(self, request,pk): # 业务逻辑：删除 return Response({'code': 0, 'data': \"...\"}) ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:10:1","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"GenericAPIView GenericAPIView 继承APIView，在APIView的基础上又增加了一些功能。例如：get_queryset、get_object等。 实际在开发中一般不会直接继承它，他更多的是担任 中间人的角色，为子类提供公共功能。 from django.urls import path, re_path, include from app01 import views urlpatterns = [ path('api/users/', views.UserView.as_view()), path('api/users/\u003cint:pk\u003e/', views.UserDetailView.as_view()), ] from rest_framework.generics import GenericAPIView from rest_framework.response import Response class UserView(GenericAPIView): queryset = models.UserInfo.objects.filter(status=True) serializer_class = 序列化类 def get(self, request): queryset = self.get_queryset() ser = self.get_serializer(intance=queryset,many=True) print(ser.data) return Response({\"code\": 0, 'data': \"...\"}) ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:10:2","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"GenericViewSet GenericViewSet类中没有定义任何代码，他就是继承 ViewSetMixin 和 GenericAPIView，也就说他的功能就是将继承的两个类的功能继承到一起。 GenericAPIView，将数据库查询、序列化类的定义提取到类变量中，便于后期处理。 ViewSetMixin，将 get/post/put/delete 等方法映射到 list、create、retrieve、update、partial_update、destroy方法中，让视图不再需要两个类。 from django.urls import path, re_path, include from app01 import views urlpatterns = [ path('api/users/', views.UserView.as_view({\"get\":\"list\",\"post\":\"create\"})), path('api/users/\u003cint:pk\u003e/', views.UserView.as_view({\"get\":\"retrieve\",\"put\":\"update\",\"patch\":\"partial_update\",\"delete\":\"destory\"})), ] # views.py from rest_framework.viewsets import GenericViewSet from rest_framework.response import Response class UserView(GenericViewSet): # 认证、权限、限流等 queryset = models.UserInfo.objects.filter(status=True) serializer_class = 序列化类 def list(self, request): # 业务逻辑：查看列表 queryset = self.get_queryset() ser = self.get_serializer(intance=queryset,many=True) print(ser.data) return Response({\"code\": 0, 'data': \"...\"}) def create(self, request): # 业务逻辑：新建 return Response({'code': 0, 'data': \"...\"}) def retrieve(self, request,pk): # 业务逻辑：查看某个数据的详细 return Response({\"code\": 0, 'data': \"...\"}) def update(self, request,pk): # 业务逻辑：全部修改 return Response({'code': 0, 'data': \"...\"}) def partial_update(self, request,pk): # 业务逻辑：局部修改 return Response({'code': 0, 'data': \"...\"}) def destory(self, request,pk): # 业务逻辑：删除 return Response({'code': 0, 'data': \"...\"})from rest_framework.viewsets import GenericViewSetfrom rest_framework.response import Response​ class UserView(GenericViewSet): # 认证、权限、限流等 queryset = models.UserInfo.objects.filter(status=True) serializer_class = 序列化类 def list(self, request): # 业务逻辑：查看列表 queryset = self.get_queryset() ser = self.get_serializer(intance=queryset,many=True) print(ser.data) return Response({\"code\": 0, 'data': \"...\"})​ def create(self, request): # 业务逻辑：新建 return Response({'code': 0, 'data': \"...\"}) def retrieve(self, request,pk): # 业务逻辑：查看某个数据的详细 return Response({\"code\": 0, 'data': \"...\"})​ def update(self, request,pk): # 业务逻辑：全部修改 return Response({'code': 0, 'data': \"...\"}) def partial_update(self, request,pk): # 业务逻辑：局部修改 return Response({'code': 0, 'data': \"...\"}) def destory(self, request,pk): # 业务逻辑：删除 return Response({'code': 0, 'data': \"...\"}) 注意：开发中一般也很少直接去继承他，因为他也属于是 中间人类，在原来 GenericAPIView 基础上又增加了一个映射而已。 ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:10:3","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"五大类 在drf的为我们提供好了5个用于做 增、删、改（含局部修改）、查列表、查单个数据的5个类（需结合 GenericViewSet 使用）。 from django.urls import path, re_path, include from app01 import views urlpatterns = [ path('api/users/', views.UserView.as_view({\"get\":\"list\",\"post\":\"create\"})), path('api/users/\u003cint:pk\u003e/', views.UserView.as_view({\"get\":\"retrieve\",\"put\":\"update\",\"patch\":\"partial_update\",\"delete\":\"destroy\"})), ] from rest_framework.viewsets import GenericViewSet from rest_framework.mixins import ( ListModelMixin, CreateModelMixin, RetrieveModelMixin, UpdateModelMixin, DestroyModelMixin, ListModelMixin ) class UserView(CreateModelMixin,RetrieveModelMixin, UpdateModelMixin, DestroyModelMixin,ListModelMixin,GenericViewSet): # 认证、权限、限流等 queryset = models.UserInfo.objects.filter(status=True) serializer_class = 序列化类 在这个5个类中已帮我们写好了 list、create、retrieve、update、partial_update、destory 方法，我们只需要在根据写 类变量：queryset、serializer_class即可。 # urls.py from django.urls import path from app01 import views urlpatterns = [ path('api/users/', views.UserView.as_view({\"get\": \"list\"})), path('api/users/\u003cint:pk\u003e/', views.UserView.as_view({\"get\": \"retrieve\"})), ] from rest_framework import serializers from rest_framework.viewsets import GenericViewSet from rest_framework import mixins from app01 import models class UserModelSerializer(serializers.ModelSerializer): level_text = serializers.CharField( source=\"get_level_display\", read_only=True ) extra = serializers.SerializerMethodField(read_only=True) class Meta: model = models.UserInfo fields = [\"username\", \"age\", \"email\", \"level_text\", \"extra\"] def get_extra(self, obj): return 666 class UserView(mixins.ListModelMixin, mixins.RetrieveModelMixin, GenericViewSet): queryset = models.UserInfo.objects.all() serializer_class = UserModelSerializer ","date":"2024-04-18","objectID":"http://localhost:1313/posts/drf/:10:4","tags":null,"title":"Drf整合","uri":"http://localhost:1313/posts/drf/"},{"categories":["default"],"content":"快速上手websocket 在dj中 #环境： python 3.9 django4.2 pip install channels pip install daphne ","date":"2024-04-18","objectID":"http://localhost:1313/posts/websocket/:0:0","tags":null,"title":"Websocket","uri":"http://localhost:1313/posts/websocket/"},{"categories":["default"],"content":"dj的使用 安装好后，我们要先知道django是通过wsgi进行启动的。但是默认情况下，wsgi是无法支持websocket的， 因此安装好后，先去setting中注册channels ``` INSTALLED_APPS = [ # \"daphne\", #有时候直接这样引入会报错，直接手动指定该app的配置即可 \"daphne.apps.DaphneConfig\", 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', ] ``` 现在基于daphne来替换wsgi来启动项目，同样在settings中 ``` # settings.py WSGI_APPLICATION = 'websocket_dj.wsgi.application' ASGI_APPLICATION = 'websocket_dj.asgi.application' ``` 然后启动就可以看到程序是通过ASGI/Daphne启动的了，此时同时支持异步和websocket协议了。 但是此时打开asgi.py 需要进行一些配置，同时需要利用channels，这需要到settings中进行配置 ``` # settings.py INSTALLED_APPS = [ \"daphne.apps.DaphneConfig\", 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', \"channels\", \"app01.apps.App01Config\" ] # asgi.py import os from django.core.asgi import get_asgi_application from channels.routing import ProtocolTypeRouter, URLRouter from .routing import websocket_urlpatterns os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'websocket_dj.settings') # application = get_asgi_application() application = ProtocolTypeRouter({ \"http\": get_asgi_application(), # 如果是http协议那么就会使用get_asgi_application() \"websocket\": URLRouter(websocket_urlpatterns), # 可以指定websocket 相应的路由 }) ``` 接下来就可以去websocket_urlpatterns中编写对应的路由了，几乎可以类似于传统的django路由 ``` # websocket 协议路由 from django.urls import path from app01 import views from app01 import views websocket_urlpatterns = [ path(\"room/\", views.ChatConsumen.as_asgi()) ] ## views.py from channels.generic.websocket import WebsocketConsumer from channels.exceptions import StopConsumer class ChatConsumen(WebsocketConsumer): def websocket_connect(self, message): print(\"有人连接了\") self.accept() def websocket_receive(self, message): print(\"接收到消息了\", message) self.send(text_data=\"收到了\") def websocket_disconnect(self, message): print(\"客户端断开连接了\") raise StopConsumer() ``` 此时通过前端脚本，建立连接并且实时准备接收后台的消息 ```html \u003cinput type=\"text\" id=\"txt\"/\u003e \u003cinput type=\"button\" value=\"发送\" onclick=\"dosend()\"/\u003e \u003cinput type=\"button\" value=\"关闭\" onclick=\"doclose()\"\u003e \u003cscript type=\"text/javascript\"\u003e // ws://127.0.0.1:8000/room/ var ws = new WebSocket(\"ws://\" + window.location.host + \"/room/\") ws.onmessage = function (event) { console.log(event.data) } function dosend() { let text = document.getElementById(\"txt\").value ws.send(text) } function doclose() { ws.close() } \u003c/script\u003e \u003c ``` ","date":"2024-04-18","objectID":"http://localhost:1313/posts/websocket/:1:0","tags":null,"title":"Websocket","uri":"http://localhost:1313/posts/websocket/"},{"categories":["default"],"content":"案例1： 聊天室 思路梳理 访问并显示 -http:127.0.0.1:8000/index/?group=1008 ``` from channels.exceptions import StopConsumer,AcceptConnection,DenyConnection,InvalidChannelLayerError from asgiref.sync import async_to_sync LIVE_DICT = { } class ChatConsumen(WebsocketConsumer): def websocket_connect(self, message): print(\"有人连接了\"), # print(\"self.scope.url_route\", self.scope['url_route']['kwargs']['group']) group = self.scope['url_route']['kwargs']['group'] LIVE_DICT.setdefault(group, []) # 如果对应聊天室不存在默认创建一个空列表 LIVE_DICT[group].append(self) # 将当前建立的这个websocket连接实例对象假如到字典，中 后续可以通过对象.send来实现对客户端发送消息 self.accept() def websocket_receive(self, message): # print(\"接收到消息了\", message) group = self.scope['url_route']['kwargs']['group'] for client in LIVE_DICT[group]: # 拿到所有的ChatConsumen对象，执行.send 把消息广播给客户端 client.send(text_data=message['text']) # self.send(text_data=\"收到了\") def websocket_disconnect(self, message): print(\"客户端断开连接了\") group = self.scope['url_route']['kwargs']['group'] LIVE_DICT[group].remove(self) # 退出聊天群，直接踢出 raise StopConsumer() ``` ","date":"2024-04-18","objectID":"http://localhost:1313/posts/websocket/:2:0","tags":null,"title":"Websocket","uri":"http://localhost:1313/posts/websocket/"},{"categories":["default"],"content":"案例2 channels 聊天室 安装channels-redis pip install channels-redis 配置CHANNEL_LAYERS ``` # settings.py CHANNEL_LAYERS = { \"default\": { \"BACKEND\": \"channels_redis.core.RedisChannelLayer\", \"CONFIG\": { \"hosts\": [(\"127.0.0.1\", 6379)] # \"hosts\": [\"redis://xxxxx密码@127.0.0.1:6379\"], } } } ``` 此时group就不需要通过group字典来进行存储了 然后对应的视图处理函数需要进行简单的修改 from channels.exceptions import StopConsumer,AcceptConnection,DenyConnection,InvalidChannelLayerError from asgiref.sync import async_to_sync # 基于channel redis class ChatConsumen(WebsocketConsumer): def websocket_connect(self, message): print(\"有人连接了\"), # print(\"self.scope.url_route\", self.scope['url_route']['kwargs']['group']) group = self.scope['url_route']['kwargs']['group'] # 将客户端对象，添加到redis中 async_to_sync(self.channel_layer.group_add)(group,self.channel_name) self.accept() def websocket_receive(self, message): # print(\"接收到消息了\", message) group = self.scope['url_route']['kwargs']['group'] async_to_sync(self.channel_layer.group_send)(group,{\"type\":\"xx.oo\",\"message\":message}) # self.send(text_data=\"收到了\") def xx_oo(self,event): text = event[\"message\"][\"text\"] self.send(text) def websocket_disconnect(self, message): print(\"客户端断开连接了\") group = self.scope['url_route']['kwargs']['group'] async_to_sync(self.channel_layer.group_discard)( # 移除内容 group,self.channel_name ) raise StopConsumer() ","date":"2024-04-18","objectID":"http://localhost:1313/posts/websocket/:3:0","tags":null,"title":"Websocket","uri":"http://localhost:1313/posts/websocket/"},{"categories":["default"],"content":"jwt算法 问题描述: ​ 传统的登录状态token，通常会保存在数据库或者缓存中，这其实增加服务器的压力，那么有没有什么方法，可以在不需要保存token，却又能让系统记住这个所谓的\"token\"呢， 当然是有的，他就是jwt。 ","date":"2024-04-17","objectID":"http://localhost:1313/posts/jwt/:0:0","tags":null,"title":"Jwt算法","uri":"http://localhost:1313/posts/jwt/"},{"categories":["default"],"content":"jwt 密文结构 # 由三段字符串组成，中间用.将其隔开 xxxxx.xxxxx.xxxx ","date":"2024-04-17","objectID":"http://localhost:1313/posts/jwt/:1:0","tags":null,"title":"Jwt算法","uri":"http://localhost:1313/posts/jwt/"},{"categories":["default"],"content":"jwt 加密思路： 假如我拿到如下用户信息 { \"username\":\"yutianwang\", \"password\":\"123123\" } jtw加密的思路其实就是分成三步，分别生成三段密文（包括根据给出用户信息的json串） ","date":"2024-04-17","objectID":"http://localhost:1313/posts/jwt/:2:0","tags":null,"title":"Jwt算法","uri":"http://localhost:1313/posts/jwt/"},{"categories":["default"],"content":"第一段密文 先注意如下json字符串，先将其进行base64url 编码 { \"alg\":\"HS256\", \"typ\":\"JWT\" } ## 使用python代码处理 import base64 import json original_data = { \"alg\":\"HS256\", \"typ\":\"JWT\" } json_data = json.dumps(original_data).encode('utf8') encoded_data = base64.b64encode(json_data).decode('utf8') print(encoded_data) 然后我们拿到了这么一个东西： eyJhbGciOiAiSFMyNTYiLCAidHlwIjogIkpXVCJ9 这个东西将来就是jwt的第一段(可以说是固定内容) ","date":"2024-04-17","objectID":"http://localhost:1313/posts/jwt/:2:1","tags":null,"title":"Jwt算法","uri":"http://localhost:1313/posts/jwt/"},{"categories":["default"],"content":"第二段密文 拿到用户信息 { \"id\":\"1234567890\", \"name\":\"John Doe\", \"time\":\"1516239022\" } 同样先进行base64url编码，拿到这个玩意 eyJpZCI6ICIxMjM0NTY3ODkwIiwgIm5hbWUiOiAiSm9obiBEb2UiLCAidGltZSI6ICIxNTE2MjM5MDIyIn0= 它就是jwt中的第二段密文 ","date":"2024-04-17","objectID":"http://localhost:1313/posts/jwt/:2:2","tags":null,"title":"Jwt算法","uri":"http://localhost:1313/posts/jwt/"},{"categories":["default"],"content":"第三段密文 第三段密文思路如下 base64url(HS256(第一段密文.第二段密文),密钥加盐) 最后将三段密文内容都用.拼接起来就的到了最终密文 ","date":"2024-04-17","objectID":"http://localhost:1313/posts/jwt/:2:3","tags":null,"title":"Jwt算法","uri":"http://localhost:1313/posts/jwt/"},{"categories":null,"content":"作者介绍 一个计算机专业的菜鸟 联系方式 qq: 1257091748 电话号码: 17879765153 邮箱: 17879765153@163.com 微信: 余天王微信\r兴趣爱好 旅游 吃 听歌 关于计算机的一切 涉猎内容 测试内容 测试内容 ","date":"2024-04-15","objectID":"http://localhost:1313/about/:0:0","tags":null,"title":"作者","uri":"http://localhost:1313/about/"},{"categories":["deploy"],"content":"关于redis各平台的安装","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["deploy"],"content":"1： 学习环境介绍： windows + vmware:centos 7 redis3.0.7 2: redis 5.0 版本安装,以及配置 下载安装 所有版本下载链接：链接1 mkdir -p /data/redis6379 cd /opt # yum -y install wget wget https://download.redis.io/releases/redis-5.0.7.tar.gz tar -zxf redis-5.0.7.tar.gz cd /opt/redis-5.0.7 make \u0026\u0026 make install 关于编译安装的命令： # 有一份二进制的软件包，想要安装，一般有以下几个步骤 ./configure .... # 洗菜和切菜，指定安装目录和配置等 make # 炒菜，编译安装软件 make install # 装盘，根据./configure的配置进行实际安装到指定位置 安装完成后，Redis可执行程序会自动添加到/usr/local/bin路径，我们就可以在任何地方的terminal中使用Redis相关命令了。 [root@cs opt]# ll /usr/local/bin/redis* -rwxr-xr-x. 1 root root 4365728 Aug 1 10:58 /usr/local/bin/redis-benchmark -rwxr-xr-x. 1 root root 8124120 Aug 1 10:58 /usr/local/bin/redis-check-aof -rwxr-xr-x. 1 root root 8124120 Aug 1 10:58 /usr/local/bin/redis-check-rdb -rwxr-xr-x. 1 root root 4806800 Aug 1 10:58 /usr/local/bin/redis-cli lrwxrwxrwx. 1 root root 12 Aug 1 10:58 /usr/local/bin/redis-sentinel -\u003e redis-server -rwxr-xr-x. 1 root root 8124120 Aug 1 10:58 /usr/local/bin/redis-server 到这一步，我们就可以在任意路径下，调用redis的相关命令了，如： [root@cs opt]# redis-server -v Redis server v=5.0.7 sha=00000000:0 malloc=jemalloc-5.1.0 bits=64 build=a67ef2d8861912e7 4. 编写配置文件 无密码配置： mkdir -p /opt/redis6379/{conf,logs,pid} cat \u003e/opt/redis6379/conf/redis6379.conf\u003c\u003cEOF daemonize yes # 注意，生产中， 千万不要bind 0.0.0.0，不要将Redis暴露到外网环境，防止被人攻击 bind 127.0.0.1 $(ifconfig ens33|awk 'NR==2{print $2}') port 6379 pidfile /opt/redis6379/pid/redis6379.pid logfile /opt/redis6379/logs/redis6379.log EOF 5. 启动命令 redis-server /opt/redis6379/conf/redis6379.conf ps -ef|grep redis 6. 客户端连接 无密码的，客户端直接连接就可以操作，并且支持远程连接： [root@cs opt]# redis-cli 127.0.0.1:6379\u003e ping PONG 127.0.0.1:6379\u003e quit [root@cs opt]# 7. 关闭命令 # 结合启动命令测试关闭命令 redis-server /opt/redis6379/conf/redis6379.conf ps -ef|grep redis # 方式1 [root@cs opt]# redis-cli 127.0.0.1:6379\u003e SHUTDOWN not connected\u003e quit [root@cs opt]# # 方式2 [root@cs opt]# redis-cli shutdown [root@cs opt]# # 方式3 [root@cs opt]# pkill -9 redis [root@cs opt]# 8. 可选的配置，配置systemd管理Redis # 先把之前可能运行的Redis停止 # 创建redis用户和组，以及给相关目录权限 # 创建systemctl管理redis的文件 # 就可以通过systemctl管理redis了 redis-cli shutdown # -u和-g选项表示同时添加具有特定UID和GID的用户 # -M创建一个没有主目录的用户 # -s表示当前创建的当前用户无法用来登录系统 # chown -R redis:redis表示指定目录以及内部的文件所有用户属组归于redis:redis # groupdel redis # cat /etc/group |grep redis groupadd redis -g 1000 # userdel redis # cat /etc/passwd |grep redis useradd redis -u 1000 -g 1000 -M -s /sbin/nologin chown -R redis:redis /opt/redis* chown -R redis:redis /data/redis* cat \u003e/usr/lib/systemd/system/redis.service\u003c\u003cEOF [Unit] Description=Redis persistent key-value database After=network.target After=network-online.target Wants=network-online.target [Service] ExecStart=/usr/local/bin/redis-server /opt/redis6379/conf/redis6379.conf --supervised systemd ExecStop=/usr/local/bin/redis-cli shutdown Type=notify User=redis Group=redis RuntimeDirectory=redis RuntimeDirectoryMode=0755 [Install] WantedBy=multi-user.target EOF systemctl daemon-reload # 当添加或者修改了某个服务的配置文件，就要执行daemon-reload命令重新加载下 systemctl start redis # 启动/停止/重启/查看状态/设置redis开机自启 systemctl start/stop/restart/status/enable redis 到此，安装成功了。 ","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/:0:0","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["deploy"],"content":"优化警告# 如果你查看redis的日志，你会发现启动，日志中会出现一些警告，我们可以针对性的优化这些警告。 cat /opt/redis6379/logs/redis6379.log ","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/:1:0","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["deploy"],"content":"警告1：maximum open files过低# 就是客户端连接数有点小了，改大点就行了 63918:M 01 Aug 2023 11:28:59.700 # You requested maxclients of 10000 requiring at least 10032 max file descriptors. 63918:M 01 Aug 2023 11:28:59.700 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted. 63918:M 01 Aug 2023 11:28:59.700 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'. 解决，systemd启动文件添加参数LimitNOFILE： cat \u003e/usr/lib/systemd/system/redis.service\u003c\u003cEOF [Unit] Description=Redis persistent key-value database After=network.target After=network-online.target Wants=network-online.target [Service] ExecStart=/usr/local/bin/redis-server /opt/redis6379/conf/redis6379.conf --supervised systemd ExecStop=/usr/local/bin/redis-cli shutdown Type=notify User=redis Group=redis RuntimeDirectory=redis RuntimeDirectoryMode=0755 LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF ","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/:1:1","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["deploy"],"content":"警告2：overcommit_memory设置# 虚拟内存相关，overcommit_memory 表内存分配策略，可选值：0、1、2 0，表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。 1， 表示内核允许分配所有的物理内存，而不管当前的内存状态如何。 2， 表示内核允许分配超过所有物理内存和交换空间总和的内存 63918:M 01 Aug 2023 11:28:59.701 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect., 解决： # 临时解决 sysctl vm.overcommit_memory=1 # 永久解决 vim /etc/sysctl.conf 追加： vm.overcommit_memory=1 # 生效配置 sysctl -p ","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/:1:2","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["deploy"],"content":"警告3：关闭THP⼤内存⻚# redis建议我们关掉THP，还给出的具体的操作办法，注意必须使用root来操作，否则会失败。 Linux kernel 在 2.6.38 的版本中新增了 THP 的特性，支持大内存页（2MB）分配，默认开启。 当开启 THP 时会降低 fork 子进程的速度，但是 fork 操作之后，每个内存页从原来 4KB 变为 2MB，会大幅增加重写期间父进程内存消耗。 同时每次写命令引起的复制内存也单位放大了 512 倍，会拖慢写操作的时间，导致大量写操作慢查询，例如：简单的 incr、set 命令也会出现在慢查询中。 63918:M 01 Aug 2023 11:28:59.701 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never \u003e /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 解决，必须以root用户执行下面的命令： # 临时解决 echo never \u003e /sys/kernel/mm/transparent_hugepage/enabled # 永久解决 vim /etc/rc.local 追加： echo never \u003e /sys/kernel/mm/transparent_hugepage/enabled ","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/:1:3","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["deploy"],"content":"警告4：TCP连接数调整# 意思是配置 /proc/sys/net/core/somaxconn的值是128，但redis.conf中配置的是511，但是linux内核会以无提示的方式将其截断为128。在一个高并发的环境下，128是远远不够的，所以我们要改大一些。 63918:M 01 Aug 2023 11:28:59.701 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 解决： # 永久解决 vim /etc/sysctl.conf 追加： net.core.somaxconn= 4096 # 生效配置 sysctl -p 然后在重启下redis，并观察日志： # 先清空原有的日志内容 # 然后重启redis # 观察日志输出 echo \"\"\u003e/opt/redis6379/logs/redis6379.log cat /opt/redis6379/logs/redis6379.log systemctl daemon-reload systemctl stop redis systemctl status redis systemctl start redis systemctl status redis cat /opt/redis6379/logs/redis6379.log # 这输出就很干净了啊 [root@cs opt]# cat /opt/redis6379/logs/redis6379.log 79069:C 01 Aug 2023 12:05:05.217 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 79069:C 01 Aug 2023 12:05:05.217 # Redis version=5.0.7, bits=64, commit=00000000, modified=0, pid=79069, just started 79069:C 01 Aug 2023 12:05:05.217 # Configuration loaded 79069:C 01 Aug 2023 12:05:05.217 * supervised by systemd, will signal readiness 79069:M 01 Aug 2023 12:05:05.218 * Running mode=standalone, port=6379. 79069:M 01 Aug 2023 12:05:05.218 # Server initialized 79069:M 01 Aug 2023 12:05:05.219 * Ready to accept connections 最终这些调整完，最好重启下服务器。然后再启动下redis就好了。 3:关于密码# 如果需要密码，用下面的配置： mkdir -p /opt/redis6379/{conf,logs,pid} cat \u003e/opt/redis6379/conf/redis6379.conf\u003c\u003cEOF daemonize yes # 注意，生产中， 千万不要bind 0.0.0.0，不要将Redis暴露到外网环境，防止被人攻击，这里为了演示方便才这么做的 bind 0.0.0.0 # requirepass 后面跟空格，空格后面是你要设置的密码 requirepass 1234 port 6379 pidfile /opt/redis6379/pid/redis6379.pid logfile /opt/redis6379/logs/redis6379.log EOF systemctl restart redis 更改了配置文件，别忘了重启Redis。 有密码的，这么连接： [root@cs opt]# redis-cli # 这里默认连接的是6379端口的Redis服务 127.0.0.1:6379\u003e ping (error) NOAUTH Authentication required. # 由于我设置了密码，所以，这里提示需要认证，如果你没有设置密码，这里应该返回PONG 来说下认证的两种方式。下面是访问的同时携带密码，连接认证一起做了。 # -a 后面跟你的密码 # -p 后面是6379端口，其实这个-p参数可以不带，不带默认就是访问的6379端口的Redis [root@cs opt]# redis-cli -a 1234 -p 6379 127.0.0.1:6379\u003e ping PONG 127.0.0.1:6379\u003e 另一种方式就是，先连接上，然后再认证： [root@cs opt]# redis-cli # 连接肯定是没问题，但由于设置了密码，你未通过认证之前，你啥也做不了 127.0.0.1:6379\u003e ping (error) NOAUTH Authentication required. 127.0.0.1:6379\u003e auth 1234 # 认证，auth命令后面跟你的密码 OK 127.0.0.1:6379\u003e ping # 认证通过，你想干啥干啥 PONG 127.0.0.1:6379\u003e quit # 常见的退出客户端命令有： quit/exit/ctrl+c 注意，退出客户端只是表示关闭客户端和服务端的连接，而不是停止服务端，这点要区分开 [root@cs opt]# 4：redis常用的全局命令： 对所有的数据类型都生效的命令。 # 高危命令，生产禁用！返回所有的redis中所有的key，尽量避免在生产中使用 KEYS * # 查询以k2开头的所有KEYS KEYS k2* # 返回指定key的值 KEYS k1 # 返回当前数据库中key的总数，这个先相当于是个计数器，生产中可以用 DBSIZE # 返回指定key的value值的类型 TYPE k1 # 删除一个key，key存在并且删除成功，返回1，否则返回0 # 删除n个key，并且都删除成功，返回删除成功的key的个数 DEL k1 DEL k1 k2 ...kn # 判断 key 是否存在,如果返回0，表示key不存在，如果判断多个key，例如，EXISTS k1 k2 k3，如果都存在，返回3 # 如果返回0表示都不存在，返回1,2的话，就表示只有这一两个key存在 EXISTS k1 EXISTS k1 k2 ... kn # 重命名 RENAME k1 k11 # 查看所有的配置项 CONFIG GET * # 以键值对的形式返回所有的配置项 CONFIG GET appendonly # 查看指定配置项 CONFIG SET requirepass 1234 # 临时动态的设置某个设置，重启redis失效，想要永久的，还是要手动的写入到配置文件中 CONFIG REWRITE # 将临时动态设置的某个设置，刷写到配置文件中 # 高危命令，生产禁用！清空redis中所有的key，找都找不回来的那种清空，如果用了，请尽早买好跑路的高铁票 FLUSHALL # 查看服务端，此时此刻的客户端连接数 INFO CLIENTS # 查看服务端支持的最大的客户端连接数 CONFIG GET maxclients 设置键值对的生存时间： # 以秒/毫秒为单位设置生存时间 EXPIRE/PEXPIRE # 以秒/毫秒为单位返回剩余的生存时间 TTL/PTTL # 即在生存时间内，取消生存时间设置 PERSIST # 为 k2 设置生存时间为20秒，关于返回值 # 0 表示key不存在 # 1 表示key存在，且设置过期时间成功 EXPIRE k2 20 # 返回 k2 剩余的生存时间，关于返回值 # -1 表示key存在且永不过期 # -2 表示key不存在 # n 表示key存在，还有n秒后过期 TTL k2 # 取消过期时间设置，或者你可以重新 set k2 v2 当然，set这种不够优雅 PERSIST k2 注意，不要将大量的键值对设置为同一时间失效，避免造成缓存雪崩！ redis存储数据的上限 由于Redis是一个基于内存的数据库，所以它的数据存储在内存中，那么Redis数据大小限制取决于内存的大小。 因此，当Redis中的数据超过可用内存时，它将开始使用交换空间（swap space）或溢出文件（overflow file），这将导致系统变慢或崩溃。因此，我们需要注意Redis中数据大小的限制，以便避免这些问题的发生。 单个键值对的大小限制：Redis最大可以存储512MB的数据，因此，单个键值对的大小不能超过512MB。超过这个大小将导致数据丢失或内存中断。 Redis数据库的大小限制：每个Redis数据库默认可以最多存储2^32个键值对，当达到这个限制后，将无法再向其添加更多数据。 ","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/:1:4","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["deploy"],"content":"禁用高危命令# 刚才学了一些命令，你可以发现有些高危命令是生产中禁止使用的，那么相信人的自觉性肯定是不靠谱的，我们需要禁用掉这些命令。 打开你的redis.conf文件，想要禁用某个命令，可以这样写： # 为某个命令赋值为空字符串，就先当于禁用掉这个命令了 # 你也可以为某个命令重命名，防止自己也没得用 rename-command KEYS \"\" # rename-command KEYS \"KEYSSS\" rename-command SHUTDOWN \"\" rename-command CONFIG \"\" rename-command FLUSHALL \"\" 你应该根据情况决定是禁用掉某个命令还是重命名某个命令。 比如你直接禁用掉到DEL命令，但此时aof文件中有DEL的操作日志，那么当你重启Redis或者调整aof文件时，就会报错，因为没有这个删除命令了….. 5:Redis5-数据类型篇 ","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/:2:0","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["deploy"],"content":"1：字符串： # 如果k1 存在，那么v1 就覆盖原来的value 值 ，如果key不存在 就是设置key value值 set k1 v1 #set key 的时候 同时设置执行过期时间 （Expiration） setex k1 10 v1 # 也可以这么写 set k1 ex v1 10 # 同时设置多个k/v mset k1 v1 k2 v2 k3 v3 # 查看对应key 的剩余存货时间(time to life) ttl k1 # 判断 k1 是否存在，如果存在返回1 不存在返回0 exists k1 # 查看对应的key的value值 get k1 # 查看多个 mget k1 k2 k3 k4 #如果k1没有，则返回一个nil并赋值，有则返回这个值并覆盖 getset k1 v1 (nil) get k1 \"v1\" # 如果key不存在，就设置一个key:value，如果key存在则什么都不做 # 返回 0 表示key存在，啥也不做 # 返回 1 表示key不存在，设置键值对成功 127.0.0.1:6379\u003e SETNX k1 v1 (integer) 0 # 删除单个key，删除多个key,并返回成功删除的个数 127.0.0.1:6379\u003e DEL k1 (integer) 1 127.0.0.1:6379\u003e DEL k1 k2 k3 (integer) 2 # 表示成功删除两个。 # 追加值，若key不存在，相当于set键值对，若key存在，将append的value拼接到原来key的value的后面 127.0.0.1:6379\u003e APPEND k1 v1 (integer) 2 127.0.0.1:6379\u003e GET k1 \"v1\" 127.0.0.1:6379\u003e APPEND k1 111 (integer) 5 127.0.0.1:6379\u003e GET k1 \"v1111\" # 为指定字节位置的元素替换为指定值从0开始的哦 127.0.0.1:6379\u003e SET k7 0123456789 127.0.0.1:6379\u003e SETRANGE k7 5 A # 将第5个字节位置的值替换为 A (integer) 10 127.0.0.1:6379\u003e GET k7 # 替换后的结果 \"01234A6789\" 127.0.0.1:6379\u003e SETRANGE k7 12 BC # 如果指定的字节数超出字符串长度，就补零 (integer) 14 127.0.0.1:6379\u003e GET k7 \"01234A6789\\x00\\x00BC\" 127.0.0.1:6379\u003e SETRANGE k7 6 7 BC # 不允许这种将第6~7字节位置的元素替换为指定值 (error) ERR wrong number of arguments for 'setrange' command # 判断key是否存在，存在返回1，否则返回0 127.0.0.1:6379\u003e EXISTS k1 (integer) 1 # 如果key存在，返回值的长度，否则返回0 127.0.0.1:6379\u003e STRLEN k1 (integer) 2 127.0.0.1:6379\u003e STRLEN k222 # k222不存在 (integer) 0 # 返回字符串指定字节范围的值 127.0.0.1:6379\u003e SET k7 0123456789 OK 127.0.0.1:6379\u003e GETRANGE k7 1 5 # 第1~5个字节范围内的值，注意，索引从0开始 \"12345\" 127.0.0.1:6379\u003e GETRANGE k7 5 15 # 第5~15个字节范围内的值，如果字符串长度不够，以起始位置开始，有多少返回多少 \"56789\" 127.0.0.1:6379\u003e GETRANGE k7 15 20 # 起始和结束范围都不在字符串范围内，返回空 \"\" 上例中有对范围设置值或者取值的操作，但谨记，不能对中文这么做，因为一个中文由多个字节组成。 计数器功能： 127.0.0.1:6379\u003e INCR num # 每次调用INCR命令，num值自加一，num提前可以不存在 (integer) 1 127.0.0.1:6379\u003e INCR num (integer) 2 127.0.0.1:6379\u003e INCRBY num 10 # 一次性加n (integer) 12 127.0.0.1:6379\u003e INCRBY num 100 (integer) 112 127.0.0.1:6379\u003e DECR num # DECR 使num自减一 (integer) 111 127.0.0.1:6379\u003e decrby num 10 # 一次性减n (integer) 101 127.0.0.1:6379\u003e DECR num1 # 如果num1这个key不存在，则先设置为0，再减一 (integer) -1 127.0.0.1:6379\u003e GET num # 可以通过GET获取该计数器的值 \"101\" 127.0.0.1:6379\u003e GET num1 \"-1\" 127.0.0.1:6379\u003e TYPE num # 注意，值仍然是字符串 string ","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/:3:0","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["deploy"],"content":"2:列表# 应用场景： 消息队列系统 最新的微博消息，比如我们将最新发布的热点消息都存储到Redis中，只有翻看\"历史久远\"的个人信息，这类冷数据时，才去MySQL中查询 列表的特点： 后插入的在最前面，相当于每次都默认在索引0前面做插入操作。这个特性相当于微信朋友圈，最新发布的的动态在最上面。 列表内每一个元素都有自己的下标索引，从左到右，从0开始；从右到左，从-1开始，这跟Python中的列表一样。 列表中的元素可重复。 # 增 LPUSH l1 a b # 如果 l1 不存在就创建，然后将 a b 插入到 l1 中，如果 l1 存在，直接将 a b 插入到 l1 中 注意 从左边插入，所以a先进去，然后b进去，所以b在a左边 LPUSHX l1 c # 如果 key 存在则插入，不存在则什么也不做 LINSERT l1 before a a1 # 在元素 a 前面插入 a1 LINSERT l1 after a a2 # 在元素 a 后插入 a2 RPUSH l1 1 # 在列表尾部追加 元素 1 RPUSH l1 2 3 # 在列表尾部先追加 2 再追加 3 ,注意这里的插入从左往右就是123了 RPUSHX l1 4 # 如果列表 l1 存在就将元素4追加到列表尾部，如果列表不存在则什么也不做 # 查，根据索引下标取值 LRANGE l1 0 -1 # 从索引0开始取到-1，也就是从头取到尾，获取列表中的所有元素 LRANGE l1 0 2 # 取索引 0 1 2 三个索引对应的元素 LRANGE l1 0 0 # 取索引 0 对应的元素 LRANGE l1 10 15 # 如果索引不在合法范围内，则取空 LINDEX l1 1 # 根据索引下标返回元素 # 删除 DEL l1 # 删除整个列表 LPOP l1 # 抛出列表头部元素 RPOP l1 # 抛出列表尾部元素 RPOPLPUSH l1 l2 # 将列表 l1 尾部的元素抛出并且添加到列表 l2 中 LREM l1 2 a1 # 从左到右，删除指定个数的元素 a1 ，本示例中，若 a1 有 1 个，就删一个，若 a1 有 3 个或者更多，也就删除 2 个 LTRIM l1 2 4 # 保留索引2-4的元素，其余删除 # 改 LSET l1 0 a # 将列表索引为0的元素修改为指定值，如果索引位置不存在则报错 ","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/:4:0","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["deploy"],"content":"3:Redis-hash# 应用场景： 作为MySQL的缓存。 对应关系： mysql数据格式： user表 id name job age 1 zhang it 18 2 wang it 24 3 zhao it 20 对应hash类型存储格式： key field1 value field2 value field3 value user:1 name zhang job it age 18 user:2 name wang job it age 18 user:3 name zhao job it age 18 user对应MySQL的表，n和MySQL中user表的id一一对应 user:n 操作： 都是H开头的命令 # 增 127.0.0.1:6379\u003e HMSET user:1 name zhangkai age 18 gender m OK 127.0.0.1:6379\u003e HMSET user:2 name wangkai age 18 gender m OK # 查 # 获取指定字段的值，相当于 select name from user where id=1; 127.0.0.1:6379\u003e HGET user:1 name \"zhangkai\" # 获取多个指定字段的值，相当于 select name,age from user where id=1; 127.0.0.1:6379\u003e HMGET user:1 name age 1) \"zhangkai\" 2) \"18\" # 获取所有键值对，相当于 select * from user; 127.0.0.1:6379\u003e HGETALL user:1 1) \"name\" 2) \"zhangkai\" 3) \"age\" 4) \"18\" 5) \"gender\" 6) \"m\" # 判断指定字段是否存在，存在返回1，否则返回0 127.0.0.1:6379\u003e HEXISTS user:1 name (integer) 1 # 返回key中所有字段的数量 127.0.0.1:6379\u003e HLEN user:1 (integer) 3 # 改 127.0.0.1:6379\u003e HINCRBY user:1 age 8 # 为整型字段的值加固定数字 127.0.0.1:6379\u003e HSET user:1 name zhangkai2 # 为指定字段重新赋值 ","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/:5:0","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["deploy"],"content":"4集合# 集合的应用场景： 适用于各种需要求交集、并集、差集的场景，比如共同好友，共同关注的场景。 另外这里的集合也有数学上的集合特性，去重，交、并、差集的运算。 基础操作： # 增 127.0.0.1:6379\u003e SADD s1 a b c # 声明key并添加 a b c 三个元素 (integer) 3 127.0.0.1:6379\u003e SMEMBERS s1 # 返回 s1 中所有元素 1) \"c\" 2) \"b\" 3) \"a\" 127.0.0.1:6379\u003e SCARD s1 # 返回 s1 中元素的个数 (integer) 3 127.0.0.1:6379\u003e SRANDMEMBER s1 # 随机返回集合中的1个元素 \"a\" 127.0.0.1:6379\u003e SRANDMEMBER s1 \"a\" 127.0.0.1:6379\u003e SRANDMEMBER s1 \"c\" 127.0.0.1:6379\u003e SRANDMEMBER s1 \"b\" 127.0.0.1:6379\u003e SRANDMEMBER s1 2 # 随机返回指定个数的元素 1) \"b\" 2) \"c\" 127.0.0.1:6379\u003e SISMEMBER s1 a # 判断元素 a 是否存在 (integer) 1 # 删除 127.0.0.1:6379\u003e SPOP s1 # 随机删除一个元素，并将这个元素返回 \"c\" 127.0.0.1:6379\u003e SPOP s1 2 # 随机删除指定个数的元素，并将多个元素返回 1) \"d\" 2) \"a\" 127.0.0.1:6379\u003e DEL s1 # 删除集合 (integer) 1 # 移动 127.0.0.1:6379\u003e SADD s1 a b c # 将元素 a 从s1移动到s2中，如果s2不存在，就先创建再移动 (integer) 3 127.0.0.1:6379\u003e SMOVE s1 s2 a # 注意，一次只能移动一个，不支持多个，比如：SMOVE s1 s2 a b c (integer) 1 127.0.0.1:6379\u003e SMEMBERS s1 1) \"c\" 2) \"b\" 127.0.0.1:6379\u003e SMEMBERS s2 1) \"a\" 再来看集合的运算。我们来个示例，有个需求，现有个培训学校开设了Python和Linux两门课程，来学习的同学都有如下情况： 有的同学学习Linux 有的学习Python 还有的既学了Linux又学了Python 那现在问题来了，我们要对这些同学的情况做统计，比如找出两门课都报了的同学？ # 先把数据准备好 127.0.0.1:6379\u003e SADD python xiaoA xiaoB Huluwa xiaoC xiaoMaque (integer) 5 127.0.0.1:6379\u003e SADD linux xiaoC xiaoMaque xiaoD xiaoE xiaoDongbei (integer) 5 # 求交集，找出即学习了Python又学习了Linux的同学 127.0.0.1:6379\u003e SINTER python linux # 这俩集合谁在前谁在后没区别 1) \"xiaoC\" 2) \"xiaoMaque\" # 求交集，找出即学习了Python又学习了Linux的同学，并将结果保存到集合tmp中，tmp不存在则先创建 127.0.0.1:6379\u003e SINTERSTORE tmp python linux (integer) 2 127.0.0.1:6379\u003e SMEMBERS tmp 1) \"xiaoMaque\" 2) \"xiaoC\" # 求并集，找出学习两门课程的所有人 127.0.0.1:6379\u003e SUNION python linux 1) \"xiaoE\" 2) \"xiaoA\" 3) \"xiaoB\" 4) \"xiaoD\" 5) \"xiaoDongbei\" 6) \"xiaoMaque\" 7) \"xiaoC\" 8) \"Huluwa\" # 求并集，找出学习两门课程的所有人，并将结果保存到集合tmp中，tmp不存在则先创建 127.0.0.1:6379\u003e SUNIONSTORE tmp python linux (integer) 8 127.0.0.1:6379\u003e SMEMBERS tmp 1) \"xiaoE\" 2) \"xiaoA\" 3) \"xiaoB\" 4) \"xiaoD\" 5) \"xiaoDongbei\" 6) \"xiaoMaque\" 7) \"xiaoC\" 8) \"Huluwa\" # 求差集，找出只学习了Python（或者Linux）课程的人 127.0.0.1:6379\u003e SDIFF python linux # 只学习Python课程的人 1) \"xiaoA\" 2) \"xiaoB\" 3) \"Huluwa\" 127.0.0.1:6379\u003e SDIFFSTORE tmp python linux # 只学习Python课程的人,并将结果保存到集合tmp中，tmp不存在则先创建 (integer) 3 127.0.0.1:6379\u003e SMEMBERS tmp 1) \"xiaoA\" 2) \"xiaoB\" 3) \"Huluwa\" 127.0.0.1:6379\u003e SDIFF linux python # 只学习Linux课程的人 1) \"xiaoDongbei\" 2) \"xiaoE\" 3) \"xiaoD\" # # 只学习Linux课程的人,并将结果保存到集合tmp中，tmp不存在则先创建 127.0.0.1:6379\u003e SDIFFSTORE tmp linux python (integer) 3 127.0.0.1:6379\u003e SMEMBERS tmp 1) \"xiaoDongbei\" 2) \"xiaoE\" 3) \"xiaoD\" ","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/:6:0","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["deploy"],"content":"5 有序集合zset# 有序集合在集合的基础上增加了排序功能，比如以点击数为条件进行排序。 有序集合的典型应用场景： 各种排行榜，音乐排行榜、热点新闻榜….. 在有序集合中，我们一般称其元素为成员，其成员对应的值为得分。 # 增加 127.0.0.1:6379\u003e ZADD top 0 xuwei 0 zhoujielun (integer) 2 # 查, withscores是可选参数，表示同时返回其成员和其得分 127.0.0.1:6379\u003e ZSCORE top xuwei # 返回top中，指定成员的分数，如果指定成员不存在则返回nil \"0\" 127.0.0.1:6379\u003e ZSCORE top zhangkai (nil) # 返回top中所有成员，默认以分数大小升序排序，分值相同按ASCII编码排序 127.0.0.1:6379\u003e ZRANGE top 0 -1 1) \"xuwei\" 2) \"zhoujielun\" 127.0.0.1:6379\u003e ZRANGE top 0 -1 WITHSCORES 1) \"xuwei\" 2) \"0\" 3) \"zhoujielun\" 4) \"0\" # 返回成员在有序集合中的索引下标，如果成员不存在返回nil 127.0.0.1:6379\u003e ZRANK top xuwei (integer) 0 127.0.0.1:6379\u003e ZRANK top zhangkai (nil) # 返回top中成员的数量 127.0.0.1:6379\u003e ZCARD top (integer) 2 # 返回top中所有成员，-inf表示第一个成员，+inf表示最后一个成员，结果默认以其分数的大小排序 127.0.0.1:6379\u003e ZRANGEBYSCORE top -inf +inf 1) \"xuwei\" 2) \"zhoujielun\" 127.0.0.1:6379\u003e ZRANGEBYSCORE top -inf +inf WITHSCORES 1) \"xuwei\" 2) \"0\" 3) \"zhoujielun\" 4) \"0\" # 返回top中所有成员，并且以索引从大到小排序 127.0.0.1:6379\u003e ZREVRANGE top 0 -1 1) \"zhoujielun\" 2) \"xuwei\" 127.0.0.1:6379\u003e ZREVRANGE top 0 -1 WITHSCORES 1) \"zhoujielun\" 2) \"0\" 3) \"xuwei\" 4) \"0\" # 返回top中 0 \u003c= index \u003c= 2 索引范围内的成员，不在范围内返回空 127.0.0.1:6379\u003e ZREVRANGE top 0 2 1) \"zhoujielun\" 2) \"xuwei\" 127.0.0.1:6379\u003e ZREVRANGE top 0 2 WITHSCORES 1) \"zhoujielun\" 2) \"0\" 3) \"xuwei\" 4) \"0\" 127.0.0.1:6379\u003e ZREVRANGE top 3 5 WITHSCORES (empty list or set) # 返回top中分数在 20 \u003c= 分数 \u003c= 200 这个范围内的成员的数量 127.0.0.1:6379\u003e ZADD top 20 xuwei 80 zhoujielun 100 daolang 120 zhaolei # 先搞个数据 127.0.0.1:6379\u003e ZCOUNT top 20 200 (integer) 4 # 返回top中分数在 20 \u003c= 分数 \u003c= 200 这个范围内的成员 127.0.0.1:6379\u003e ZRANGEBYSCORE top 20 200 1) \"xuwei\" 2) \"zhoujielun\" 3) \"daolang\" 4) \"zhaolei\" 127.0.0.1:6379\u003e ZRANGEBYSCORE top 20 200 WITHSCORES 1) \"xuwei\" 2) \"20\" 3) \"zhoujielun\" 4) \"80\" 5) \"daolang\" 6) \"100\" 7) \"zhaolei\" 8) \"120\" # 返回top中分数范围在 100 \u003c= 的 \u003c= 20 范围内的成员，并以的的大小降序排序 127.0.0.1:6379\u003e ZREVRANGEBYSCORE top 100 20 1) \"daolang\" 2) \"zhoujielun\" 3) \"xuwei\" 127.0.0.1:6379\u003e ZREVRANGEBYSCORE top 100 20 WITHSCORES 1) \"daolang\" 2) \"100\" 3) \"zhoujielun\" 4) \"80\" 5) \"xuwei\" 6) \"20\" # 改 # 为指定成员重新赋值 127.0.0.1:6379\u003e ZADD top 30 xuwei (integer) 0 # 为指定成员增加指定分数，并返回增加后的分数 127.0.0.1:6379\u003e ZINCRBY top 10 xuwei \"40\" # 删除 # 删除分数在指定范围内的成员，并返回删除成员的数量 127.0.0.1:6379\u003e ZREMRANGEBYSCORE top 80 100 (integer) 2 # 删除top中指定索引范围内 0 \u003c= index \u003c= 1 的成员 127.0.0.1:6379\u003e ZREMRANGEBYRANK top 0 1 (integer) 2 # 删除top中一个或者多个成员，返回实际删除的个数，没有找到key返回0 127.0.0.1:6379\u003e ZREMRANGEBYRANK top 0 1 (integer) 2 127.0.0.1:6379\u003e DEL top (integer) 1 来个音乐排行榜的示例： # 创建初始的音乐排行榜，初始每首歌播放量都是0 ZADD top 0 aiqing 0 guxiang 0 lanlianhua 0 shiguang # 模拟每首歌的播放量 ZINCRBY top 123 aiqing ZINCRBY top 223 guxiang ZINCRBY top 203 lanlianhua ZINCRBY top 103 shiguang # 返回播放量前三名的歌名 127.0.0.1:6379\u003e ZREVRANGE top 0 2 WITHSCORES 1) \"guxiang\" 2) \"223\" 3) \"lanlianhua\" 4) \"203\" 5) \"aiqing\" 6) \"123\" 6：redis-发布订阅 消息队列 Redis中的发布订阅模式，类似于RabbitMQ中的主题订阅模式。 在发布订阅模式中，有三个角色： 生产者，它同样是一个redis客户端，负责生产消息。可以有一个或者多个生产者往一个或者多个频道中发布消息。 channel，频道，类似于主题，接收来自于生产者产生的消息，为不同的消息打个标签，可以有多个频道。它维护在redis的server中。 消费者，也是一个Redis客户端，可以订阅感兴趣的消息，也就是可以(一个或者多个)订阅不同频道的消息。 如上图，有三个生产者负责生产消息，并且发布到指定的频道中，消费者可以选择订阅一个或者多个频道中的消息。 操作： # 首先要现有多个消费者订阅不同的频道，订阅一个、多个、模糊匹配；你可以起三个终端来分别订阅\rSUBSCRIBE fm103\rSUBSCRIBE fm103 fm104\rPSUBSCRIBE fm*\r# 然后有不同的生产者往指定的频道生产消息，可以用三个终端来发布消息，也可以用一个也行，分别向不同的频道发布消息\rPUBLISH fm103 \"fm103.9 it's my radio, messages1\"\rPUBLISH fm103 \"fm103.9 it's my radio, messages2\"\rPUBLISH fm103 \"fm103.9 it's my radio, messages3\"\rPUBLISH fm104 \"fm104.9 it's my radio, messages1\"\rPUBLISH fm104 \"fm104.9 it's my radio, messages2\"\rPUBLISH fm104 \"fm104.9 it's my radio, messages3\"\rPUBLISH fm105 \"fm105.9 it's my radio, messages1\"\rPUBLISH fm105 \"fm105.9 it's my radio, messages2\"\rPUBLISH fm105 \"fm105.9 it's my radio, messages3\" 上述几个消费者就能根据自己的规则接收到不同的频道的消息，其效果如下图 这里需要注意的是，消费者不会接收频道的历史消息，只会接受订阅后产生的消息。 其他操作： # 取消订阅频道 UNSUBSCRIBE # 不跟频道号则取消订阅所有频道 UNSUBSCRIBE fm103 # 取消订阅fm103频道 UNSUBSCRIBE fm103 fm104 # 取消订阅多个频道 UNSUBSCRIBE fm* # 取消订阅以fm开头的频道 Redis发布订阅优点： Redis的发布订阅支持多个生产者/消费者，同时生产消费消息，优点就是非常简洁，因为它的实现原理就是单纯地为生产者、消费者建立「数据转发通道」，把符合规则的数据，从一端转发到另一端。 缺点: 发布订阅模式是\"发后既忘\"的","date":"2024-04-14","objectID":"http://localhost:1313/posts/redis/:7:0","tags":null,"title":"redis安装","uri":"http://localhost:1313/posts/redis/"},{"categories":["spider"],"content":"这里推荐三种可行的识别图片验证码的方法 超级鹰 直接从官网引入其sdk import base64 import requests from lxml import etree from hashlib import md5 url = \"https://so.gushiwen.cn/user/login.aspx\" headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36', } response = requests.get(url=url, headers=headers).text tree = etree.HTML(response) img_src = \"https://so.gushiwen.cn\" + tree.xpath('//img[@id=\"imgCode\"]/@src')[0] img_content = requests.get(url=img_src, headers=headers).content with open('code6.jpg', 'wb') as fp: fp.write(img_content) # # print(img_src) #!/usr/bin/env python # coding:utf-8 class Chaojiying_Client(object): def __init__(self, username, password, soft_id): self.username = username password = password.encode('utf8') self.password = md5(password).hexdigest() self.soft_id = soft_id self.base_params = { 'user': self.username, 'pass2': self.password, 'softid': self.soft_id, } self.headers = { 'Connection': 'Keep-Alive', 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)', } def PostPic(self, im, codetype): \"\"\" im: 图片字节 codetype: 题目类型 参考 http://www.chaojiying.com/price.html \"\"\" params = { 'codetype': codetype, } params.update(self.base_params) files = {'userfile': ('ccc.jpg', im)} r = requests.post('http://upload.chaojiying.net/Upload/Processing.php', data=params, files=files, headers=self.headers) return r.json() def PostPic_base64(self, base64_str, codetype): \"\"\" im: 图片字节 codetype: 题目类型 参考 http://www.chaojiying.com/price.html \"\"\" params = { 'codetype': codetype, 'file_base64':base64_str } params.update(self.base_params) r = requests.post('http://upload.chaojiying.net/Upload/Processing.php', data=params, headers=self.headers) return r.json() def ReportError(self, im_id): \"\"\" im_id:报错题目的图片ID \"\"\" params = { 'id': im_id, } params.update(self.base_params) r = requests.post('http://upload.chaojiying.net/Upload/ReportError.php', data=params, headers=self.headers) return r.json() # 实例化一个超级鹰 chaojiying = Chaojiying_Client('17879765153', 'yuxin4161010', '952346') im = open('code6.jpg','rb') # 图片二进制数据 data = chaojiying.PostPic(im, 5000) print(type(data),data['pic_str']) DDDDOCR 比较简单的图片验证码建议使用这个 pip install ddddocr==1.4.9 pip install pillow==9.5.0 本地识别 import ddddocr ocr = ddddocr.DdddOcr(show_ad=False) with open(\"code.jpg\", mode=\"rb\") as f: body = f.read() code = ocr.classification(body) print(code) 在线识别 import ddddocr import requests res = request.get(url=xxxxx) ocr = ddddocr.Dd获取请求相应的ddocr(show_ad=False) # 请求相应的二进制数据交给其识别 code = ocr.classification(res.content) print(code) 云打码 云打码官网提供了多种类型的图片验证码的识别 云打码官网 # 引入sdk class YdmVerify(object): # 自定义验证码识别服务的API地址 _custom_url = \"http://api.jfbym.com/api/YmServer/customApi\" # 认证token _token = \"QBBoSN0szS0kWg1c33XdGPKZmUJJOlzpUh4tykRUcNY\" # 请求头 _headers = { 'Content-Type': 'application/json' } # 定义获取验证码的方法 def get_verify_code(self, image): # 构造请求参数 payload = { \"image\": image, \"token\": self._token, \"type\": \"10110\", } # 发送POST请求，获取验证码识别结果 resp = requests.post(self._custom_url, headers=self._headers, data=json.dumps(payload)) # 解析响应结果，提取验证码识别结果 return resp.json().get('data', {}).get('data', '') # 图片链接。 img_data = \"xxxxxxx\" # 对图片进行base64编码 base46_img_data = base64.b64encode(img_data).decode('utf8') ydm = YdmVerify() code = ydm.get_verify_code(base46_img_data) upper_code = code.upper() print(f'扫描结果是：{upper_code}',type(upper_code)) ","date":"2024-02-16","objectID":"http://localhost:1313/posts/demo02/:0:0","tags":null,"title":"图片验证码识别","uri":"http://localhost:1313/posts/demo02/"},{"categories":["web"],"content":"概要 vue3-cookie 使用 cookies vue3中需要使用cookies存储用户信息，可以使用vue3-cookies 安装 npm install vue3-cookies 全局挂载 # main.js import App from './App.vue' import VueCookies from \"vue3-cookies\"; const app = createApp(App) app.use(VueCookies) 基本的使用 # xxxx.js import {useCookies} from 'vue3-cookies' const {cookies} = useCookies() cookies.set(\"info\", JSON.stringify(info), 10) // 设置键值对 cookies.get(\"info\") //获取键值对，可以直接获取json对象，不需要通过parse进行解析 pinia import {ref, computed} from 'vue' import {defineStore} from 'pinia' import {useCookies} from 'vue3-cookies' const {cookies} = useCookies() // 导入vue3使用cookies export const userInfoStore = defineStore('counter', () =\u003e { var userStr = ref(cookies.get(\"info\")) const userDict = computed(() =\u003e JSON.parse(userStr.value)) function doSave(info) { cookies.set(\"info\", JSON.stringify(info), 10) // 单位分钟 userStr.value = JSON.stringify(info) } return {userDict, doSave} }) 导航守卫 router.beforeEach(function (to, from, next) { // 1 登录页面，不需要校验直接过 if (to.name === \"login\") { next() return; } const store = userInfoStore() console.log(\"store\",store) if (!store.userId) { next({name: \"login\"}) }else{ next() } }) element-ui 配置 npm install element- import ElementPlus from 'element-plus' import 'element-plus/dist/index.css' app.use(ElementPlus) //全局注册element-plus 登录组件 模板部分 \u003ctemplate\u003e \u003c!-- \u003cv-form-designer\u003e\u003c/v-form-designer\u003e--\u003e \u003cdiv class=\"box\" v-if=\"!isRegister\"\u003e \u003ch1 style=\"text-align: center\"\u003e用户登录\u003c/h1\u003e \u003cel-form :model=\"form\" label-width=\"auto\" style=\"max-width: 600px\" label-position=\"top\" :rules=\"formRules\" ref=\"formRef\"\u003e \u003cel-form-item label=\"用户名\" :error=\"formError.username\" prop=\"username\"\u003e \u003cel-input v-model=\"form.username\" style=\"width: 240px\" placeholder=\"用户名\"/\u003e \u003c/el-form-item\u003e \u003cel-form-item label=\"密码\" :error=\"formError.password\" prop=\"password\"\u003e \u003cel-input v-model=\"form.password\" style=\"width: 240px\" placeholder=\"请输入密码\"/\u003e \u003c/el-form-item\u003e \u003c/el-form\u003e \u003cel-form-item\u003e \u003cel-button type=\"primary\" @click=\"doSubmit\"\u003e登录\u003c/el-button\u003e \u003cspan style=\"margin-left: 10px\" @click=\"isRegister=true\"\u003e没有账号?点击注册\u003c/span\u003e \u003c/el-form-item\u003e \u003c/div\u003e \u003cdiv class=\"box\" v-if=\"isRegister\"\u003e \u003ch1 style=\"text-align: center\"\u003e用户注册\u003c/h1\u003e \u003cel-form :model=\"registerForm\" label-width=\"auto\" style=\"max-width: 600px\" label-position=\"top\" :rules=\"registerRules\" ref=\"formRef\"\u003e \u003cel-form-item label=\"上传头像\" prop=\"avator\"\u003e \u003cel-upload class=\"avatar-uploader\" action=\"http://127.0.0.1:8000/upload/\" :show-file-list=\"false\" :on-success=\"handleAvatarSuccess\" :before-upload=\"beforeAvatarUpload\" \u003e \u003cimg v-if=\"imageUrl\" :src=\"imageUrl\" class=\"avatar\" style=\"width: 150px;height: 120px\" /\u003e \u003cimg v-if=\"!imageUrl\" :src=\"static_url + `/media/avatar/default.jpg`\" class=\"avatar\" style=\"width: 150px;height: 120px\" /\u003e \u003cel-icon v-else class=\"avatar-uploader-icon\"\u003e\u003cPlus /\u003e\u003c/el-icon\u003e \u003c/el-upload\u003e \u003c/el-form-item\u003e \u003cel-form-item label=\"昵称\" :error=\"RegisterError.nickname\" prop=\"nickname\"\u003e \u003cel-input v-model=\"registerForm.nickname\" style=\"width: 240px\" placeholder=\"昵称\"/\u003e \u003c/el-form-item\u003e \u003cel-form-item label=\"用户名\" :error=\"RegisterError.username\" prop=\"username\"\u003e \u003cel-input v-model=\"registerForm.username\" style=\"width: 240px\" placeholder=\"用户名\"/\u003e \u003c/el-form-item\u003e \u003cel-form-item label=\"密码\" :error=\"RegisterError.password\" prop=\"password\"\u003e \u003cel-input v-model=\"registerForm.password\" style=\"width: 240px\" placeholder=\"请输入密码\"/\u003e \u003c/el-form-item\u003e \u003c!-- \u003cel-form-item label=\"性别\"\u003e--\u003e \u003c!-- \u003cel-select v-model=\"registerForm.sex\" placeholder=\"请选择性别\"\u003e--\u003e \u003c!-- \u003cel-option label=\"保密\" value=\"2\"/\u003e--\u003e \u003c!-- \u003cel-option label=\"男\" value=\"1\"/\u003e--\u003e \u003c!-- \u003cel-option label=\"女\" value=\"0\"/\u003e--\u003e \u003c!-- \u003c/el-select\u003e--\u003e \u003c!-- \u003c/el-form-item\u003e--\u003e \u003c/el-form\u003e \u003cel-form-item\u003e \u003cel-button type=\"primary\" @click=\"doRegister\"\u003e注册\u003c/el-button\u003e \u003cspan style=\"margin-left: 10px\" @click=\"isRegister=false;registerForm.avator='/media/avator/default.jpg'\"\u003e立即登录\u003c/span\u003e \u003c/el-form-item\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript setup\u003e import {ref} from \"vue\"; import {useRoute, useRouter} from \"vue-router\" import {userInfoStore} from \"@/stores/counter.js\"; import _axios from \"@/plugins/axios.js\"; import {ElMessag","date":"2024-01-28","objectID":"http://localhost:1313/posts/vue/:0:0","tags":null,"title":"Vue学习","uri":"http://localhost:1313/posts/vue/"},{"categories":null,"content":"关于redis各平台的安装","date":"2023-10-15","objectID":"http://localhost:1313/posts/first_post/","tags":null,"title":"关于博客","uri":"http://localhost:1313/posts/first_post/"},{"categories":null,"content":"第一篇文章 自从开始学计算机，我就一直对于知识点的掌握虚浮经常需要翻阅文献资料和笔记,故建此博客，以供自己查询方便，顺便熟悉一个非常牛的静态页面生成器-HUGO ","date":"2023-10-15","objectID":"http://localhost:1313/posts/first_post/:0:0","tags":null,"title":"关于博客","uri":"http://localhost:1313/posts/first_post/"},{"categories":["spider"],"content":"项目启动 创建工程 scrapy startproject 工程名称 进入到项目内的spiders文件夹中，创建爬虫文件。 scrapy genspider spiderName www.xxx.com 执行程序 scrapy crawl siderName # 执行爬虫程序 scrapy crawl siderName --nolog # 无日志执行爬虫程序 爬虫文件的组成 创建好一个爬虫文件里面有一些为我们创建好的代码 import scrapy class FirstSpider(scrapy.Spider): name = \"first\" allowed_domains = [\"www.xxx.com\"] start_urls = [\"https://www.xxx.com\"] def parse(self, response): pass name 爬虫文件的名称，就是爬虫源文件的唯一标识。 allowed_domains 允许的域名，用来限定start_urls 列表中哪些可以进行请求发送。可以把它注释掉，就没有限定了。 start_urls 起始url的列表，里面的url会被scrapy自动进行请求的发送。也就是说你想要对哪个url发起请求，放到列表里就行了 不需要requests.get。。。。 手动发起请求了。 def parse(self,response) 用于数据解析的函数,response 参数 每次对url发起请求后就会调用这个parse方法 就表示 请求成功的响应对象。 配置文件的一些设置 LOG_LEVEL = \"ERROR\" # 只显示错误类型日志 USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\" # ua伪装 ROBOTSTXT_OBEY = False # roborts协议 关于数据解析 方法一： 直接通过 response.xpath 就可以解析响应数据，无需etree对象 def parse(self, response, **kwargs): # 爬取房名，面积 # 直接可以使用xpath解析方法 divs = response.xpath('//section[@class=\"list\"][1]/div') for div in divs: title = div.xpath('./a/div[2]/div[1]/div[1]/h3/text()').extract_first() print(title) 方法二 ： 创建选择器对象，可以做更多类型的解析比如css def parse(self, response, **kwargs): sel = scrapy.Selector(response) divs = sel.xpath('//section[@class=\"list\"][1]/div') for div in divs: title = div.xpath('./a/div[2]/div[1]/div[1]/h3/text()').extract_first() print(title) 注意，无论用哪种，都需要要通过.extract_first() 方法 才能拿到纯净的数据。 如果能保证拿到的只有一个列表元素，那么就适合用.extract_first() 关于数据持久化存储 方式一： 基于终端指令 要求： 只可以将parse方法的返回值，存储到本地的文本文件中。 all_data = [] # 解析方式2 sel = scrapy.Selector(response) divs = sel.xpath('//section[@class=\"list\"][1]/div') dict ={} for div in divs: title = div.xpath('./a/div[2]/div[1]/div[1]/h3/text()').extract_first() # 价格 + 单位 price = div.xpath('.//section[@class=\"list\"][1]/div//span[@class=\"property-price-total-num\"]/text()').extract_first() + div.xpath('.//section[@class=\"list\"][1]/div//span[@class=\"property-price-total-text\"]/text()') dict = { '标题':title, '价格':price } all_data.append(dict) return all_data # 把数据封装在列表中，返回 ，这样我们就可以基于终端指令来存储这个列表 # 配置文件进行配置 FEED_FORMAT = \"csv\" # FEED_URI = \"results.csv\" # # 终端输入 ： scrapy crawl ershou1 -o ./ershou.csv 方式二： 基于管道持久化数据存储。 编码流程。 数据解析：parse函数中把相关的数据进行解析并且提取出来 在item.py 中定义item类。 将解析出来的数据封装存储在item类型的对象中， 注意：爬虫文件如果导入不了item，去设置里面把根目录设为源路径 将item类型的对象提给管道进行持久化存储的操作。 在管道类中的process_item中将其接收到的 item对象中存储的数据 进行持久化存储。 在配置文件中开启管道。 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:0:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"示例： ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:1:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"演示定义item import scrapy class ErshoufangItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() title = scrapy.Field() price = scrapy.Field() ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:1:1","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"演示封装item from ershoufang.items import ErshoufangItem # 导入item ..... # 实例化一个item 对象 item = ErshoufangItem() item['title'] = title item['price'] = price yield item # 将item提交给了管道。 每次循环封装一组，提交给管道。 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:1:2","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"演示处理管道 def process_item(self, item, spider): ## 获取数据 title = item['title'] price = item['price'] 关于管道 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:1:3","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"最重要的方法 process_item 此方法一般执行数据下载，存储的操作。 def process_item(self, item, spider): ..... 接收item 并处理 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:2:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"开启爬虫执行的方法 def open_spider(self,spider): ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:3:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"结束爬虫的时候执行的方法 def close_spider(self,spider): ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:4:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"手写一个管道用于存储到mysql # 新建一个管道 class mysqlPipeline: def open_spider(self,spider): pass def process_item(self, item, spider): pass def close_spider(self, spider): pass ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:5:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"详细操作： open_spider： # 建立数据库连接 def open_spider(self,spider): self.conn = pymysql.connect(host='127.0.0.1',user='root',password='4161010',port=3306,database='spider',charset=\"utf8\") process_item： self.cursor = self.conn.cursor() # 建立游标对象 try: self.cursor.execute(\"insert into ershoufang(title,price)values(%s,%s)\",(item['title'],item['price'])) self.conn.commit() except Exception as e: print(e) self.conn.rollback() #如果有异常事务回滚 close_item: def close_spider(self, spider): self.cursor.close() self.conn.close() 注意：# 爬虫文件提交的item类型对象最终流向何方 ，给的一定是先执行的管道类，然后这个管道要记得return item 以把item传递给下一个管道。 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:5:1","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"面试问题： 将爬取得到的数据一份一份存储到数据库，如何实现？ 管道文件中一个管道类对应将数据存储到一个平台 爬虫文件提价的item只会给管道文件中第一个被执行的管道类 process_item 中的return item 标识将item传递给下一个即将被执行的管道类。 全站数据爬取 就是将网站中某板块下的全部页码对应的而数据进行爬取。 爬取彼岸图网的汽车模块图片名称。 实现方式： 多个请求的发送。 方法一： 把所有的url，都扔到start_urls列表中(不推荐：如果几十页几百页，显然就不现实了) 方法二： 自行手动进行请求发送。(强烈推荐) # 调用parse再次对new_url 发起请求，进行数据解析 yield scrapy.Request(url=new_url,callback=self.parse) # 先对第一页的数据进行爬取 import scrapy class MnSpider(scrapy.Spider): name = \"mn\" # allowed_domains = [\"www.xxx.com\"] start_urls = [\"https://pic.netbian.com/4kmeinv/\"] # 基于起始url对应的数据解析操作。 def parse(self, response,**kwargs): sel = scrapy.Selector(response) # 拿到图片名字对应的列表 lis = sel.xpath('//div[@class=\"slist\"]/ul/li') for li in lis: name = li.xpath('.//b/text()').extract_first() # 拿到每一个图片的名称 print(name) page = 2 ....... def parse(self, response,**kwargs): # 然后定制后面页面的url，并且对其手动进行访问 # 对第二页手动请求发送,指定回调函数为: self.parse 函数 if self.page \u003c= 11: new_url = f\"https://pic.netbian.com/4kmeinv/index_{self.page}.html\" self.page += 1 yield scrapy.Request(url=new_url,callback=self.parse) callback 标识回调函数，执行数据解析的。url是发起请求的url 关于框架： ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:6:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"五大核心组件 调度器 ： 过滤器：1：将引擎提交的请求对象进行去重 ，然后将经过去重的请求对象交给队列 队列：1：请求对象去重后，在此处排队等候 管道 ： 4：拿到item，进行数据持久化存储。 引擎 ：1：会拿到spider封装的请求对象，交给调度器 2： 接收调度器队列中的请求对象交给下载器去互联网下载 3：把response给到spider进行处理。4：收到spider封装的item，交给管道进行持久化存储 下载器： 2：拿到请求对象去互联网下载 3：拿到response响应结果 交给引擎 spider ： 作用1：产生url并对url进行请求发送， 作用2：通过回调函数 parse进行数据解析。 4：对数据进行解析和封装item对象，然后给引擎 互联网（不属于组件） ：根据请求请求返回response 给下载器 请求传参： 在发起请求的时候传递一个数据值。 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:7:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"使用场景： 爬取解析数据不在同一张页面中。（深度爬取 需求： 爬取boss直聘的岗位名称，岗位描述。 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:8:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"meta : 请求传参： meta = {} 可以将meta字典传递给请求对应的回调函数 yield scrapy.Request(url,callback=self.xxxx,meta={aaa:xxx}......) 回调函数接收参数 def parse_detail(self,response): item = response.meta['aaa'] # 接收到meta传递过来的字典。 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:9:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"params: yield scrapy.Request(url=request_data.ajax_url, params=params, headers=request_data.headers, callback=self.parse_ajax_response,meta={'form':'shabi'}) # params 作为get请求参数 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:10:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"formdata: yield scrapy.FormRequest(url=request_data.ajax_url, formdata=request_data.formdata, headers=request_data.headers, callback=self.parse_ajax_response) # formdata 作为post请求的参数 图片数据爬取 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:11:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"ImagePipeline ： 是一个在scrapy框架中专门用于图片数据爬取的管道。 基于scrapy爬取字符串类的数据和图片的数据区别： 字符串： 只需要基于xpath解析，且提交管道进行持久化存储。 图片: 同过xpath只能解析到图片的src属性值。 单独对图片地址发起请求，获取图片二进制类型的数据 。然后进行持久化存储。 ImagePipeline： 只需要将img的src属性值进行解析，提交到管道，管道就会对图片的src进行请求，获取图片二进制类型的数据 。然后进行持久化存储。 使用流程 ： 来一个需求：爬取图片 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:12:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"1:解析数据 class ImgSpider(scrapy.Spider): name = \"img\" # allowed_domains = [\"https://sc.chinaz.com\"] # start_urls = [\"https://sc.chinaz.com/tupian/\"] start_urls = [\"https://pic.netbian.com/4kqiche/\"] def parse(self, response, **kwargs): sel = scrapy.Selector(response) # 拿到标签 imgs = sel.xpath('//div[@class=\"slist\"]/ul[@class=\"clearfix\"]//img') for img in imgs: # 在img基础上拿到src img_src = img.xpath(\"./@src\").extract_first() item = ImgproItem() item[\"src\"] = img_src yield item ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:13:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"2:管道下载图片 可以直接用requests发起请求拿到二进制数据，写入本地。 也可以基于一个图片下载管道 ImagesPipeline。自动下载 一：导入父类 from scrapy.pipelines.images import ImagesPipeline 二：手动封装 一个图片下载管道.,继承ImagesPipeline 要重写三个父类方法： get_media_requests 负责接收item对象，并且发起图片的url请求 file_path : 负责指定每一个图片的名字，通过return imgName 返回 item_completed ： 负责把item对象交给下一个管道类 import scrapy from scrapy.pipelines.images import ImagesPipeline class imgsPipeline(ImagesPipeline): # 根据图片地址 进行图片数据请求 def get_media_requests(self, item, info): yield scrapy.Request(item['src']) # 用于指定图片进行持久化存储的路径,和图片名称 def file_path(self,request, response=None, info=None, item=None): imgName = request.url.split('/')[-1] return imgName def item_completed(self, results, item, info): return item # 返回给下一个即将被执行的管道类 注意： 可以在配置文件里指定基础路径然后 file_path里面路径就是基于基础路径拼接上去，本质主要还是指定图片的名字 xxx.jpg 或者xxx.png之类的。 IMAGES_STORE = \"./imgs\" 关于图片名字 ； 在file_path 里面指定，一般请况是无法拿到item 的，但是注意到file_path 里面的request 其实就是get_media_requests 里面定制的对图片发起访问的请求对象，所以可以用请求传参来处理 （如下操作即可） class imgsPipeline(ImagesPipeline): def get_media_requests(self, item, info): # 对图片的高清链接发起请求 yield scrapy.Request(item['img_src'],meta={'item':item}) def file_path(self, request, response=None, info=None, item=None): # request 参数就是图片请求的请求对象，所以 item = request.meta['item'] # 在这里可以访问并操作item对象的其他字段 imgName = item['img_name'] return imgName def item_completed(self, results, item, info): return item # 返回给下一个即将被执行的管道类 关于中间件： 在scrapy中一共又有两个中间件。 在引擎和下载器之间有一个 下载中间件 在引擎和spider文件 中间有个爬虫中间件 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:14:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"下载中间件(重要)： class MiddleproDownloaderMiddleware: 作用：下载中间件可以批量拦截scrapy中的所有请求和响应 拦截请求： 进行ua伪装（非全局） 设定代理ip(为每一个请求) 拦截响应： 篡改 响应数据/响应对象 比如页面有动态加载数据，仅靠scrapy发起的请求是那不到响应数据 ​ 主要关注三个请求。 process_reqest 负责拦截请求的中间件 process_response 负责拦截响应的中间件 process_exception 拦截发生 异常的额请求对象 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:15:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"基于下载中间件的局部ua伪装 对于ua伪装，适合在process_reqest 方法里面对每一个请求进行拦截，然后随机赋予一个ua伪装 先准备一个ua池 user_agent_list = [ \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 \" \"(KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\", \"Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 \" \"(KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11\", \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 \" \"(KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6\", \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 \" \"(KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6\", \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 \" \"(KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1\", \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 \" \"(KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5\", \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 \" \"(KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5\", \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 \" \"(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 \" \"(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 \" \"(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\", \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 \" \"(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\", \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 \" \"(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\", \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 \" \"(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\", \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 \" \"(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\", \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 \" \"(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\", \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 \" \"(KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3\", \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 \" \"(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\", \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 \" \"(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\" ] 随机从ua池里取一个作ua伪装 def process_request(self, request, spider): # 准备一个ua池 user_agent_list = [......] request.headers['User-Agent'] = random.choice(user_agent_list) ​ ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:15:1","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"基于中间件的代理ip ​ 对于代理ip： # 准备两个代理池 PROXY_http = [ '153.180.102.104:80', '195.208.131.189:56055', ] PROXY_https = [ '120.83.49.90:9000', '95.189.112.214:35508', ] class MiddleproSpiderMiddleware: def process_exception(self, request, exception, spider): # Called when a download handler or a process_request() # (from other downloader middleware) raises an exception. # 对拦截的请求,拿到其url，并分割到请求协议是http还是https ，然后给它分配对应的请求。 if request.url.split(':')[0] == 'http': request.meta[\"proxy\"] = 'http://' + random.choice(self.PROXY_http) else: request.meta[\"proxy\"] = 'https://' + random.choice(self.PROXY_https) # 将修正之后的请求兑现能给你 return request ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:15:2","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"拦截响应数据： 篡改响应数据，响应对象 需求：爬取网易新闻的新闻数据（标题和对应内容） 1 ： 通过网易新闻首页解析出几个板块对应的url（无动态加载） 2 ： 通过解析到每一个模块的url在每一个板块内部爬取新闻标题和内容（动态加载) 第一步的实现： import scrapy class WanyiSpider(scrapy.Spider): name = \"wanyi\" allowed_domains = [\"www.xxx.com\"] start_urls = [\"https://www.xxx.com\"] # 存储四个板块对应详情页的url models_urls = [] # 解析五大板块对应的url def parse(self, response, **kwargs): sel = scrapy.Selector(response) # 拿到所有的li标签 li_lisst = sel.xpath('//div[@class=\"ns_area list\"]/ul/li') # 目标标签的下标列表 国内，国际，军事，航空 alist = [1, 2, 4, 5] for index in alist: # 拿到四个板块的url model_url = li_lisst[index].xpath('./a/@href').extract_first() # url装入数据容器 self.models_urls.append(model_url) 拿到了对应板块的url，接下来依次对每一个板块页面发起请求 # 尝试对每一个板块url进行请求发送 for url in self.models_urls: # parse_model 用于解析每个细节板块的回调方法 yield scrapy.Request(url=url,callback=self.parse_model) 定义回调函数 问题来了，页面是动态加载的。如何处理 分析： 目前一共发送了五次请求，一次初始url，四次详情页面的url，后面四次拿到的响应数据，是没有动态加载内容的，但是他们从网上下载下来会经过下载中间件，我们可以通过下载中间件的process_response拦截下来并进行内容的篡改。 def process_response(self, request, response, spider):# spider表示爬虫类实例化的对象 # 负责拦截到四大板块的请求数据并且进行篡改 # 考虑到 一共发送了五次请求，所以需要挑选处四大板块对应的响应数据进行篡改。 # 可以通过响应对象对应的请求url 。当前方法的request参数就是拦截到的响应对象对应的请求参数 # 通过request请求对象来指定响应对象。 if request.url in spider.models_urls: # 判断当前拦截相应对象的请求url是否是四大板块的url之一 # response # 四大板块对应的响应对象 # 针对定位到的这些response进行篡改 # 如何获取动态加载出来的新闻数据呢？ selenium来获取, bro = spider.bro# 获取了在爬虫类 中示例化的浏览器对象 bro.get(request.url) # 对五个板块对应的url进行请求发送 time.sleep(2) # 等待2秒加载 page_text = bro.page_source #这就包含了动态加载的新闻数据 # 实例化一个新的响应对象 url 表示：响应对象对应的请求对象的url body表示响应数据 encoding表示： 编码 request 表示：请求对象，不变就是reqeust new_response = HtmlResponse(url=request.url,body=page_text,encoding='utf8',request=request) return new_response else: # response # 其它请求对应的响应对象 # 正常返回响应对象即可 return response 至此 ，拿到了一个新的响应对象（篡改完毕），接下里就是在回调函数里进行数据解析。 # 专门用于解析每一个板块页面中对应的新闻详情页的url def parse(self, response, **kwargs): sel = scrapy.Selector(response) # 拿到所有的li标签 li_lisst = sel.xpath('//div[@class=\"ns_area list\"]/ul/li') # 目标标签的下标列表 国内，国际，军事，航空 alist = [1, 2, 4, 5] for index in alist: # 拿到四个板块的url model_url = li_lisst[index].xpath('./a/@href').extract_first() # url装入数据容器 self.models_urls.append(model_url) # 尝试对每一个板块url进行请求发送 for url in self.models_urls: # parse_model 用于解析每个细节板块的回调方法 yield scrapy.Request(url=url,callback=self.parse_model) # 专门用于解析每一个板块页面中对应的新闻详情页的url def parse_model(self,response,**kwargs): # 每一个板块对应新闻标题内容都是动态加载的。 sel = scrapy.Selector(response) div_list = sel.xpath('//div[@class=\"ndi_main\"]/div') for div in div_list: title = div.xpath('./div/div/h3/a/text()').extract_first() news_detail_url = div.xpath('./div/div/h3/a/href').extract_first() item = WangyiproItem() # 实例化一个item item['title'] = title # 对新闻详情页的url发起请求 yield scrapy.Request(url=news_detail_url,callback=self.parse_detail,meta={'item':item}) #请求传参，只封装了一半的item交给回调函数接着封装 def parse_detail(self,response,**kwargs):# 用于解析新闻内容 sel = scrapy.Selector(response) content = sel.xpath('//*[@id=\"content\"]/div[2]//text()').extract() # 多个列表元素同时获取不要用extract_first() item = response.meta['item'] # 至此，标题和内容都已经拿到了,而且不在同一个回调函数，说明需要用到请求传参 content = ''.join(content) item['content'] = content yield item # 提交给管道持久化存储 # 爬虫文件结束的时候关闭浏览器。 def close(self, spider, reason): self.bro.close() # 爬虫结束的时候 # 关闭webdriver ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:16:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"注意：要记得开启中间件 DOWNLOADER_MIDDLEWARES = { \"middlePro.middlewares.MiddleproDownloaderMiddleware\": 543, } 最后，持久化存储网易新闻 import pymysql class WangyiproPipeline: def open_spider(self,spider): self.conn = pymysql.connect(user='root',password='4161010',port=3306,database='spider',host='127.0.0.1',charset='utf8') self.cursor = self.conn.cursor() def process_item(self, item, spider): # print(item) title = item['title'] content = item['content'].strip() # 去掉换行和空格 sql = 'insert into news(title,content)values(%s,%s);' value = (title,content) self.cursor.execute(sql,value) return item def close_spider(self,spider): self.conn.commit() self.cursor.close() self.conn.close() 关于selenium在scrapy中的注意事项 首先在爬虫类里实例化浏览器对象 在中间件通过spider参数调用对应的浏览器驱动进行对浏览器的自动化操作 一般用于篡改响应数据： 如下篡改 # 如何获取动态加载出来的新闻数据呢？ selenium来获取, bro = spider.bro# 获取了在爬虫类 中示例化的浏览器对象 bro.get(request.url) # 对五个板块对应的url进行请求发送 time.sleep(2) # 等待2秒加载 page_text = bro.page_source #这就包含了动态加载的新闻数据 # 实例化一个新的响应对象 url 表示：响应对象对应的请求对象的url body表示响应数据 encoding表示： 编码 request 表示：请求对象，不变就是reqeust new_response = HtmlResponse(url=request.url,body=page_text,encoding='utf8',request=request) return new_response CrawlSpider 是Spider的一个子类 全站数据爬取的方式。 基于spider 手动请求 基于crawlspider ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:17:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"CrawlSpider的基本使用 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:18:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"创建工程： 创建一个工程 scrapy startproject xxxx 创建爬虫文件（CrawlSpider）：CrawlSpider作为Spider scrapy genspider -t crawl 爬虫文件名称 www.xxx.com 可以看到创建成功 import scrapy from scrapy.linkextractors import LinkExtractor from scrapy.spiders import CrawlSpider, Rule class SunSpider(CrawlSpider): name = \"sun\" allowed_domains = [\"www.xxx.com\"] start_urls = [\"https://www.xxx.com\"] rules = (Rule(LinkExtractor(allow=r\"Items/\"), callback=\"parse_item\", follow=True),) def parse_item(self, response): item = {} #item[\"domain_id\"] = response.xpath('//input[@id=\"sid\"]/@value').get() #item[\"name\"] = response.xpath('//div[@id=\"name\"]').get() #item[\"description\"] = response.xpath('//div[@id=\"description\"]').get() return item 可以看到，spider文件是继承的CrawlSpider ，并且和传统的spider文件有些许的不同，以下是比较总要的两个模块 LinkExtractor 链接提取器 Rule 规则解析器 rules = (Rule(LinkExtractor(allow=r\"Items/\"), callback=\"parse_item\", follow=True),) # 实例化了一个Rule对象，也可叫他规则解析器 ,在內部有实例化了一个LinkExtractor，称之为连接提取器 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:18:1","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"链接提取器： 提取链接：根据指定规则进行指定链接的提取 LinkExtractor(allow=r'index_(\\d+).html') # 用正则表达式来提取对应的链接，如果提取到的url不完整，规则解析器会自动把访问的url拼接上解析到的url ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:18:2","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"规则解析器： callback 表示解析规则的函数， 把链接提取器提取到链接进行指定规则的解析。 # 规则解析器 将链接提取器提取到的链接进行指定规则的解析操作。 rules = (Rule(LinkExtractor(allow=r'index_(\\d+).html'), callback=\"parse_item\", follow=False),) ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:18:3","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"完整示例： import scrapy from scrapy.linkextractors import LinkExtractor from scrapy.spiders import CrawlSpider, Rule class SunSpider(CrawlSpider): name = \"sun\" # allowed_domains = [\"www.xxx.com\"] start_urls = [\"https://pic.netbian.com/4Kdujia/\"] link = LinkExtractor(allow=r'index_?(\\d?).html') # 规则解析器 将链接提取器提取到的链接进行指定规则的解析操作。 rules = (Rule( link,callback=\"parse_item\", follow=False),) def parse_item(self, response): print(response) 缺点： 显然，这个链接提取器只是作用在起始url的页面上，并不能拿到所有的链接(毕竟每一个页面都要加载出几百几千个链接显然是不合理的) 解决方案： 非常简单，把follow改为True即可 fllow = True 表示： 可以将链接提取器继续作用到 链接提取器的链接提取到的链接所对应的页面中。 import scrapy from scrapy.linkextractors import LinkExtractor from scrapy.spiders import CrawlSpider, Rule class SunSpider(CrawlSpider): name = \"sun\" # allowed_domains = [\"www.xxx.com\"] start_urls = [\"https://pic.netbian.com/4Kdujia/\"] link = LinkExtractor(allow=r'index_?(\\d?).html') # 规则解析器 将链接提取器提取到的链接进行指定规则的解析操作。 rules = (Rule( link,callback=\"parse_item\", follow=True),) # 让后续链接提取器拿到的url继续作用链接提取器进行解析 def parse_item(self, response): print(response) ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:18:4","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["spider"],"content":"完整案例： # 需求： 爬取图片的名字，以及详情页面的2k图片 # 分析: 爬取的数据并不在同一个页面中常规方法应该是通过请求传参来解决 - 让链接提取器1把所有页码链接都拿到。 - 让链接提取器2提取所有图片的详情页的链接。 - 把两个提取器拿到的数据封装到一个item里面，但是两个回调函数用不了请求传参 定义链接提取器和规则解析器 定义两个提取器和解析器。 class SunSpider(CrawlSpider): name = \"sun\" # allowed_domains = [\"www.xxx.com\"] start_urls = [\"https://pic.netbian.com/4Kdujia/\"] link = LinkExtractor(allow=r'index_?(\\d*).html') link_detail = LinkExtractor(allow=r'tupian/(\\d+).html') # 规则解析器 将链接提取器提取到的链接进行指定规则的解析操作。 rules = (Rule(link, callback=\"parse_item\", follow=True), Rule(link_detail, callback=\"parse_detail\"), ) 数据解析 我们希望的item应该是名字和图片链接，但是这两个 数据不在同一个回调函数，如果是唱过的spider方法，可以通过请求传参meta={。。。。。。} 的方式来，但是在CrawlSpider 中用不了请求传参，思路一就定义两个item，单独上传各自部分的数据。 # 如下两个解析方法是不能实现请求传参的。 # 只能分别存储到两个item中。 def parse_item(self, response): # 解析tp名字的回调函数 sel = scrapy.Selector(response) liList = sel.xpath('//div[@class=\"slist\"]/ul/li') for li in liList: img_name = li.xpath('.//img/@alt').extract_first() + '.jpg' item = SunproItem() item['img_name'] = img_name yield item def parse_detail(self, response): # 解析图片详情页的回调函数 sel = scrapy.Selector(response) img2k = \"https://pic.netbian.com\" + sel.xpath('//a[@id=\"img\"]/img/@src').extract_first() item = DetailItem() item['img2k'] = img2k yield item 定义两个item class SunproItem(scrapy.Item): # define the fields for your item here like: img_name = scrapy.Field() class DetailItem(scrapy.Item): img2k = scrapy.Field() 管道中可以判断item是哪一个 class SunproPipeline: def process_item(self, item, spider): #如何判定item的类型 if item.__class__.__name__ == 'SunproItem': print(item[\"img_name\"]) else: print(item['img2k']) return item 分布式爬虫 概念： 需要搭建一个分布式机群，让其对一组资源进行分布联合爬取。 作用： 提升爬取数据的效率 如何实现分布式： 安装一个scrapy-redis组件 pip install scrapy-redis 原生的scrapy 是不能实现分布式爬虫的。结合着scrapy-redis 就可以实现分布式爬虫 ","date":"2023-10-12","objectID":"http://localhost:1313/posts/scripy/:19:0","tags":null,"title":"Scripy菜鸟使用","uri":"http://localhost:1313/posts/scripy/"},{"categories":["default"],"content":"git 命令整理 ","date":"2023-10-11","objectID":"http://localhost:1313/posts/git/:0:0","tags":null,"title":"git命令整理","uri":"http://localhost:1313/posts/git/"},{"categories":["default"],"content":"基础命令 进入待管理文件夹 git init // 初始化让git管理当前文件夹 git status // 检测当前文件的状态 比如是否有修改，是否未管理 git add // 指定某个文件让git管理它 // 红色文件(工作区)表示新增的或者修改的文件 绿色(暂存区)文件表示已经被管理起来但是没有被生成版本的文件 git reset HEAD 文件名 // 从暂存区回到工作区，相当于撤销git add命令的提交 git commit -m \"版本号\" // 生成版本 (版本库) ","date":"2023-10-11","objectID":"http://localhost:1313/posts/git/:1:0","tags":null,"title":"git命令整理","uri":"http://localhost:1313/posts/git/"},{"categories":["default"],"content":"回滚相关 回滚之前的版本 git log // 查看所有版本记录 git reset --hard 版本哈希号 // 回滚后 git log 将无法查看当前版本之后的版本了 回滚当前版本之后的版本 git reflog // 查看提交记录 cd6d6d8 (HEAD -\u003e master) HEAD@{0}: reset: moving to cd6d6d82ecf420cf72a333572bc41b1360187541 163df79 HEAD@{1}: commit: v2 cd6d6d8 (HEAD -\u003e master) HEAD@{2}: commit (initial): v1 git reset --hard 163df79 // 再次回到之前的版本 ","date":"2023-10-11","objectID":"http://localhost:1313/posts/git/:2:0","tags":null,"title":"git命令整理","uri":"http://localhost:1313/posts/git/"},{"categories":["default"],"content":"分支相关 git 默认主干线 master ,分支之间的代码是隔离的，单独提交各自的版本即可 查看分支 git branch // 查看当前所处分支以及其它分支 创建分支 git branch dev // 创建一个新的分支名未dev 切换分支 git checkout dev // 切换到dev分支 删除分支 git branch -d dev // 删除dev 分支 合并分支（可能会产生冲突,需要找到冲突，手动修改） git marge dev // 把bug分支合并到主分支 ","date":"2023-10-11","objectID":"http://localhost:1313/posts/git/:3:0","tags":null,"title":"git命令整理","uri":"http://localhost:1313/posts/git/"},{"categories":["default"],"content":"代码推送相关 绑定仓库地址 git remote add origin \u003c仓库地址名\u003e 推送仓库 git push -(u) origin master ","date":"2023-10-11","objectID":"http://localhost:1313/posts/git/:4:0","tags":null,"title":"git命令整理","uri":"http://localhost:1313/posts/git/"},{"categories":["default"],"content":"从仓库拉代码 下载代码 git clone \u003c代码仓库地址\u003e 更新本地代码 git pull origin master //更新代码 ","date":"2023-10-11","objectID":"http://localhost:1313/posts/git/:5:0","tags":null,"title":"git命令整理","uri":"http://localhost:1313/posts/git/"},{"categories":null,"content":"基础知识 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:0:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"1.列表推导式 # 列表推导式, 格式如下 list = [逻辑值 for x in xxx ] 其实就是一个循环成成列表元素的方法 list1 = [x*x for x in range(10)] print(list1) # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] list2 = [x*2*2 for x in range(10)] print(list2) # [0, 4, 8, 12, 16, 20, 24, 28, 32, 36] list3 = [x*2 for x in list2] print(list3) # [0, 8, 16, 24, 32, 40, 48, 56, 64, 72] ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:1:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"2.装饰器 def decro(func): print('装饰器被定义了') # 只要定义了函数绑定了次修饰器就会执行。 def wrapper(*args,**kwargs): # 只有在被修饰函数被调用的时候才会执行 print(\"wrapper开始执行\") return func() return wrapper print('start') @decro def test(): print('test') print('end') test() ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:2:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"迭代器 # 可迭代对象： 可以被循环访问，或者遍历的对象比如一个列表 可迭代对象必须实现__iter__() 方法。 面试题目整理 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:3:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"1.下面这些是什么意思：@classmethod ，@staticmethod，@property？ @classmethod：@classmethod 用于定义一个类方法，类方法与普通方法不同之处在于第一个参数是类本身（通常被命名为 cls），而不是实例对象。 @staticmethod：@staticmethod 用于定义一个静态方法，静态方法与实例方法类似，但是它不接受自动传递的参数（实例或类），需要手动传递参数。 @property：@property 用于将一个方法转换为属性，使得该方法可以像访问属性一样使用，而不需要使用方法调用的括号。常用于定义 Getter 方法，用于获取属性值。 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:4:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"2.输入一个字符串，输出这个字符串的所有组合 用python实现 def all_combinations(s): result = [] def backtrack(comb, start): result.append(comb) for i in range(start, len(s)): backtrack(comb + s[i], i + 1) backtrack('', 0) return result input_str = input(\"请输入一个字符串：\") combinations = all_combinations(input_str) for comb in combinations: print(comb) ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:5:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"3.谈谈你所知道的python web框架 Django：Django 是一个开放源代码的 Web 应用框架，由 Python 编写而成，它遵循了 MVC（模型-视图-控制器）的设计模式，提供了大量的功能，包括 ORM（对象关系映射）、表单处理、用户认证、管理界面等等。Django 的易用性和完善的文档使得它成为开发 Web 应用的热门选择。 Flask：Flask 是一个轻量级的 Python Web 框架，它也使用了 Werkzeug 和 Jinja2 这两个工具库。相比于 Django，Flask 的设计更加灵活，它的核心只包含了一个基本的路由器和模板引擎，其他功能都需要通过扩展来实现。Flask 非常适合于快速开发小型到中型的 Web 应用。 FastAPI：FastAPI 是一个相对较新的 Python Web 框架，它基于 Python 3.6+ 的标准 typing 模块实现了快速的 API 开发，性能也非常优秀。通过使用类型提示，FastAPI 能够自动生成文档，并提供交互式 API 文档。这使得开发者可以方便地理解和测试 API。 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:6:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"4.使用python将数据库中student表的提取的数据写入到db.txt文件中 import sqlite3 # 连接到 SQLite 数据库 conn = sqlite3.connect('your_database.db') cursor = conn.cursor() # 查询 student 表的数据 cursor.execute('SELECT * FROM student') rows = cursor.fetchall() w # 将数据写入到 db.txt 文件中 with open('db.txt', 'w') as file: for row in rows: file.write(f'{row}\\n') # 关闭数据库连接 conn.close() ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:7:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"5.编程实现斐波那契数列，需要用递归实现 def fibonacci(n): if n \u003c= 0: return \"请输入大于0的整数\" elif n == 1: return 0 elif n == 2: return 1 else: return fibonacci(n - 1) + fibonacci(n - 2) # 输入要计算的斐波那契数列的第 n 项 n = 10 # 例如计算第 10 个斐波那契数 # 调用递归函数计算第 n 项斐波那契数 result = fibonacci(n) print(f\"斐波那契数列第 {n} 项为：{result}\") ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:8:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"6.简述join left 和join right的区别 在数据库中，左连接（LEFT JOIN）和右连接（RIGHT JOIN）是两种常见的连接操作，用于联合查询两个表的数据。它们之间的主要区别在于连接操作时左表和右表的处理方式不同。 左连接（LEFT JOIN）：左连接返回左表中的所有行，以及右表中满足连接条件的行。如果右表中没有匹配的行，则返回 NULL 值。换句话说，左连接会保留左表中的所有数据，即使在右表中没有相匹配的数据，也会显示左表中的数据。 右连接（RIGHT JOIN）：右连接返回右表中的所有行，以及左表中满足连接条件的行。如果左表中没有匹配的行，则返回 NULL 值。右连接实际上就是左连接的镜像，它会保留右表中的所有数据，即使在左表中没有相匹配的数据，也会显示右表中的数据。 简单来说，左连接保留左表中的所有数据，右连接保留右表中的所有数据。根据应用场景和数据需求，可以选择使用左连接或右连接来实现不同的查询逻辑，以获取需要的联合数据结果。 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:9:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"7.谈谈python mvc 并写出示例代码 在 Python 中，MVC（Model-View-Controller）是一种常见的软件架构模式，用于将应用程序的不同功能模块分离开来，以提高代码的可维护性和可扩展性。在 MVC 架构中，通常将应用程序分为三个部分：Model（模型）、View（视图）和Controller（控制器）。 Model（模型）：负责处理应用程序的数据和业务逻辑。模型通常包括数据存储、数据操作和业务规则等功能。 View（视图）：负责用户界面的展示和用户的交互。视图显示数据给用户，并接收用户的输入。 Controller（控制器）：负责处理用户的输入、控制应用程序的流程，以及更新模型和视图之间的交互。 下面是一个简单的示例代码，演示了如何在 Python 中使用 MVC 架构： # Model class UserModel: def __init__(self, name, age): self.name = name self.age = age def get_user_info(self): return f\"Name: {self.name}, Age: {self.age}\" # View class UserView: def display_user_info(self, user_info): print(user_info) # Controller class UserController: def __init__(self): self.model = UserModel(\"Alice\", 30) self.view = UserView() def show_user_info(self): user_info = self.model.get_user_info() self.view.display_user_info(user_info) # Main if __name__ == \"__main__\": controller = UserController() controller.show_user_info() 在这个示例代码中，模型类 UserModel 表示用户数据和业务逻辑，视图类 UserView 负责显示用户信息，控制器类 UserController 控制用户信息的展示过程。在 Main 部分，实例化了控制器对象并调用其方法来展示用户信息。这样，通过 MVC 架构可以将用户数据、展示和交互逻辑分离开来，提高代码的组织性和可扩展性。 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:10:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"8.简述HTTP协议下，一个Http请求的数据结构，简述http 和https的区别 在 HTTP 协议下，一个 HTTP 请求由请求行、请求头和请求体三部分组成。 请求行：包含请求的方法、请求的资源路径和协议版本。常见的请求方法包括 GET、POST、PUT、DELETE 等，资源路径指定要访问的资源地址，协议版本一般为 HTTP/1.1。 请求头：包含了一系列的键值对，用于传输请求相关的信息。常见的请求头包括 Host（目标主机地址）、User-Agent（客户端信息）、Content-Type（请求体类型）、Accept（可接受的数据类型）、Cookie（客户端保存的信息）等。 请求体：对于 POST、PUT 等需要传递数据的请求，请求体用于承载要发送的数据。请求体的内容格式取决于请求的 Content-Type，可以是文本、JSON、XML 等格式。 HTTP 和 HTTPS 的主要区别在于安全性和加密通信方式： HTTP：是不安全的超文本传输协议，信息是明文传输的。因此，存在被窃听和篡改的风险。对于一些敏感信息的传输，不建议使用 HTTP 协议。 HTTPS：是在 HTTP 基础上加入了 SSL/TLS 协议的安全版本，也称为 HTTP Secure。HTTPS 在数据传输过程中使用 SSL/TLS 加密算法，确保数据的机密性、完整性和认证性。使用 HTTPS 可以有效防止中间人攻击、窃听和数据篡改，提供更高的安全性。HTTPS 默认使用 443 端口进行通信。 因此，一般情况下，对于涉及用户隐私或涉密信息的传输，建议使用 HTTPS 协议以确保数据的安全性和保密性 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:11:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"9.python2 和python3 的区别 Python 2 和 Python 3 是 Python 编程语言的两个主要版本，它们之间有一些重要的区别，主要包括以下几点： 语法差异： print 语句：Python 2 使用 print 语句，而 Python 3 使用 print() 函数。 整数除法：Python 2 中的整数除法默认会截断小数部分，Python 3 中的整数除法返回浮点数结果。 输入函数：Python 2 中的输入函数是 raw_input()，Python 3 中的输入函数是 input()。 Unicode 支持： Python 3 默认使用 Unicode 字符串，而 Python 2 默认使用 ASCII 字符串，在 Python 2 中需要使用 u 前缀来表示 Unicode 字符串。 print 函数： Python 3 的 print() 函数可以接受多个参数，并且可以通过 end、sep 参数控制输出格式，而 Python 2 的 print 语句只能输出一个对象，并且不支持这些参数。 整数类型： Python 3 的整数类型只有 int，而 Python 2 中有 int 和 long 两种整数类型。在 Python 3 中整数会根据需要自动转换为长整型。 异常处理： Python 3 中的异常处理语法更加简洁明了，例如使用 as 关键字来捕获异常信息。 range() 函数： Python 3 中的 range() 函数返回的是一个迭代器，可以用于节省内存，而 Python 2 中 range() 返回的是一个列表。 总的来说，Python 3 在各方面对语言进行了改进和优化，支持更好的 Unicode 处理、语法规范和更好的语言设计。因此，新的项目应该优先选择 Python 3，而已有的使用 Python 2 开发的项目可以逐步迁移到 Python 3，以获得更好的支持和特性。 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:12:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"10.影响服务器请求QBS的核心问题有哪些 影响服务器请求 QPS（Queries Per Second，每秒查询数）的核心问题有很多，主要包括以下几点： 服务器硬件性能：服务器硬件的性能直接影响了处理请求的能力，包括 CPU 的性能、内存大小、硬盘速度等。服务器硬件性能越好，能够处理的请求量就越大。 网络带宽：服务器的网络带宽限制了服务器能够处理的请求量，如果网络带宽不足，会导致请求响应时间延长或者丢包。 请求处理方式：服务器的请求处理方式（如同步处理、异步处理、多线程、多进程等）对 QPS 有着直接影响。合理选择和优化请求处理方式可以提高服务器的吞吐量。 代码质量和优化：服务器端代码的质量和性能优化程度直接影响了请求的处理效率。高效的算法和代码可以提高服务器的并发处理能力。 数据库性能：如果请求涉及数据库操作，数据库的性能也是影响服务器 QPS 的重要因素。数据库的性能优化、索引设计以及连接池的配置都会影响服务器的请求处理能力。 缓存机制：合理使用缓存可以减轻服务器对数据库的访问压力，提高请求的处理速度。缓存的命中率和缓存更新策略等都会直接影响服务器的 QPS。 高可用和负载均衡：采用负载均衡方案可以将请求均衡的分发到多台服务器上，提高整个系统的请求处理能力和可靠性。 综上所述，影响服务器请求 QPS 的核心问题涉及到硬件性能、网络带宽、请求处理方式、代码质量、数据库性能、缓存机制等多个方面，需要综合考虑并进行合理的优化和调整，以提高服务器的性能和稳定性。 影响服务器请求 QPS（Queries Per Second，每秒查询数）的核心问题有很多，主要包括以下几点： 服务器硬件性能：服务器硬件的性能直接影响了处理请求的能力，包括 CPU 的性能、内存大小、硬盘速度等。服务器硬件性能越好，能够处理的请求量就越大。 网络带宽：服务器的网络带宽限制了服务器能够处理的请求量，如果网络带宽不足，会导致请求响应时间延长或者丢包。 请求处理方式：服务器的请求处理方式（如同步处理、异步处理、多线程、多进程等）对 QPS 有着直接影响。合理选择和优化请求处理方式可以提高服务器的吞吐量。 代码质量和优化：服务器端代码的质量和性能优化程度直接影响了请求的处理效率。高效的算法和代码可以提高服务器的并发处理能力。 数据库性能：如果请求涉及数据库操作，数据库的性能也是影响服务器 QPS 的重要因素。数据库的性能优化、索引设计以及连接池的配置都会影响服务器的请求处理能力。 缓存机制：合理使用缓存可以减轻服务器对数据库的访问压力，提高请求的处理速度。缓存的命中率和缓存更新策略等都会直接影响服务器的 QPS。 高可用和负载均衡：采用负载均衡方案可以将请求均衡的分发到多台服务器上，提高整个系统的请求处理能力和可靠性。 综上所述，影响服务器请求 QPS 的核心问题涉及到硬件性能、网络带宽、请求处理方式、代码质量、数据库性能、缓存机制等多个方面，需要综合考虑并进行合理的优化和调整，以提高服务器的性能和稳定性。 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:13:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"11. python如何处理一秒五百次的日志写操作 处理一秒五百次的日志写操作可以采取以下几种方式来提高写操作的效率： 批量写入： 将日志数据批量积累一定量后再进行写入，减少频繁的磁盘操作，提高写入效率。可以考虑利用缓冲区来暂存一定量的日志数据，然后进行批量写入。 异步写入： 将日志写入操作异步化，单独开启一个线程或进程来处理日志写入操作，主线程继续处理其他逻辑。这样可以避免因为日志写入操作阻塞主线程，提高程序的整体响应速度。 使用日志队列： 可以考虑使用消息队列来缓存日志数据，然后由另外的消费者进程负责将日志写入到文件或数据库中，以减轻主程序的负担。 优化写入文件： 使用合适的日志写入方式和工具，比如 Python 中的 logging 模块提供了一些高效的写入日志的方式，可以选择适合的日志处理器和格式，以提高写入效率。 利用缓存： 如果日志写入的目标是文件或者数据库，可以考虑使用缓存机制，将数据缓存在内存中一定时间后再批量写入磁盘，减少频繁的磁盘操作。 优化磁盘性能： 如果日志写入频繁造成了磁盘 I/O 压力过大，可以考虑使用 SSD 等高性能硬盘，或者使用 RAID 技术来提高磁盘的读写速度和容量。 综合利用上述方法，可以有效提高 Python 程序处理一秒五百次日志写入操作的效率和性能，从而更好地满足实际需求。 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:14:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"12.一张数据表，每天300万的数据增长量，怎么保证一年后的查询稳定 要确保在每天增长 300 万条数据的情况下，一年后仍然可以保证查询的稳定性，可以考虑以下几个方面进行优化： 合理设计数据表结构： 使用合适的数据表结构、索引和分区等数据库设计技巧，以提高查询效率。确保表结构规范化、冗余数据最小化，避免不必要的连接操作等。 适时清理历史数据： 对于不再需要的历史数据，可以定期进行清理或者归档，避免数据量过大导致查询效率降低。可以根据业务需求将历史数据归档到其他存储或者数据仓库中。 优化查询语句： 编写高效的查询语句，使用合适的索引、优化器提示等技术，避免全表扫描和不必要的性能开销，以提高查询效率。 水平和垂直分片： 如果数据量过大，可以考虑对数据进行水平或垂直分片，将数据分散存储在不同的节点或表中，以减轻单个节点的压力并提高查询效率。 使用缓存： 可以考虑使用缓存来缓存热点数据或查询结果，减少对数据库的频繁访问，提高查询响应速度。 数据库优化： 定期对数据库进行性能优化，包括参数调优、统计信息更新、空间回收、定期重建索引等，以保持数据库的高性能状态。 扩展数据库服务器： 如果单台数据库服务器无法满足需求，可以考虑通过主从复制、集群、分布式数据库等方式扩展数据库服务器，提高查询的并发处理能力。 综合考虑以上方面的优化方法，可以有效地保证在每天增长 300 万数据的情况下，一年后依然能够保证查询的稳定性和高效性。需要根据具体业务需求和数据量规模来选择适合的优化策略，以保持系统的稳定性和性能。 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:15:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"13.简述python import 的执行流程 当 Python 解释器执行到 import 语句时，会按照一定的执行流程来查找、加载和执行被导入的模块。以下是 import 的执行流程简述： 查找模块： Python 解释器首先会在内建模块（built-in module）和内置模块（built-in module）中查找是否存在要导入的模块。如果没有找到，则会继续搜索 sys.path 变量中指定的目录，包括当前目录、标准库目录以及其他用户自定义目录。 编译模块： 一旦找到了要导入的模块，Python 解释器会对该模块进行编译，生成对应的字节码文件（.pyc 文件），以便后续执行。 执行模块： 执行导入的模块时，Python 会创建一个新的命名空间（namespace）用来存放模块中定义的变量、函数和类等。模块中的代码会被执行，并将其定义的内容保存在这个命名空间中。 建立引用： 一旦导入模块完成，Python 解释器会建立一个模块对象，使得当前模块可以通过该对象访问导入的模块，从而实现模块间的引用和调用。 总的来说，Python 的 import 执行流程包括查找模块、编译模块、执行模块和建立引用等步骤。通过这个流程，Python 实现了模块化的代码组织方式，使得代码的复用和管理更加方便和灵活。 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:16:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"14. python中用map函数将[1, 2, 3, 4, 5]改为[1, 4, 9, 16, 25]，并使用列表推导式提取大于10的数 numbers = [1, 2, 3, 4, 5] squared_numbers = map(lambda x: x ** 2, numbers) print(list(squared_numbers)) # [1, 4, 9, 16, 25] print([num for num in list(squared_numbers) if num \u003e 10]) ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:17:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"15.正则匹配不是以4 和7结尾的手机号 import re # 定义手机号列表 phone_numbers = ['13812345678', '15678901234', '13987654327', '18345678907', '17098765432'] # 匹配不以4和7结尾的手机号的正则表达式 pattern = r'^1\\d{9}(?![47])$' # 遍历手机号列表，匹配不以4和7结尾的手机号 valid_phone_numbers = [number for number in phone_numbers if re.match(pattern, number)] print(valid_phone_numbers) ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:18:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"16. 举例几个魔术方法及其作用 在 Python 中，魔术方法（Magic methods）是以双下划线 __ 开头和结尾的特殊方法，用于实现类的特殊行为。以下是几个常用的魔术方法及其作用的示例： __init__(self, ...): 初始化方法，用于创建实例时进行初始化操作。 class MyClass: def __init__(self, value): self.value = value obj = MyClass(10) __str__(self): 返回对象的字符串表示，通常在使用 print() 函数时被调用。 class MyClass: def __init__(self, value): self.value = value def __str__(self): return f\"Value: {self.value}\" obj = MyClass(10) print(obj) # 输出 \"Value: 10\" __len__(self): 返回对象的长度，通常在调用 len() 函数时被调用。 class MyList: def __init__(self, elements): self.elements = elements def __len__(self): return len(self.elements) __getattr__(self, name): 当访问一个对象的属性不存在时，会调用 __getattr__() 方法，可用于动态返回属性的值。 class MyObject: def __init__(self): self.data = {'key1': 'value1', 'key2': 'value2'} def __getattr__(self, name): if name in self.data: return self.data[name] else: raise AttributeError(f\"'MyObject' object has no attribute '{name}'\") obj = MyObject() print(obj.key1) # 输出 'value1' __setattr__(self, name, value): 当给对象的属性赋值时，会调用 __setattr__() 方法，可用于控制属性的赋值行为。 class MyObject: def __init__(self): self.data = {} def __setattr__(self, name, value): print(f\"Setting attribute '{name}' with value '{value}'\") self.data[name] = value obj = MyObject() obj.key = 'value' # 输出 \"Setting attribute 'key' with value 'value'\" ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:19:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"17. 设计一个支持push pop top 操作的并且能在短时间内检索到最小元素的栈 class MinStack: def __init__(self): self.stack = [] self.min_stack = [] def push(self, val): self.stack.append(val) if not self.min_stack or val \u003c= self.min_stack[-1]: self.min_stack.append(val) def pop(self): if self.stack: if self.stack[-1] == self.min_stack[-1]: self.min_stack.pop() return self.stack.pop() def top(self): if self.stack: return self.stack[-1] def get_min(self): if self.min_stack: return self.min_stack[-1] # 测试 min_stack = MinStack() min_stack.push(-2) min_stack.push(0) min_stack.push(-3) print(min_stack.get_min()) # 输出 -3 min_stack.pop() print(min_stack.top()) # 输出 0 print(min_stack.get_min()) # 输出 -2 ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:20:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"18. 简述yield 和yield from ","date":"2023-06-06","objectID":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/:21:0","tags":null,"title":"面试准备","uri":"http://localhost:1313/posts/%E9%9D%A2%E8%AF%95/"}]